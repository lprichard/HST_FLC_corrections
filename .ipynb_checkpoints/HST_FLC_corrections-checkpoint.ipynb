{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unique-smooth",
   "metadata": {},
   "source": [
    "# Additional Corrections to HST FLCs\n",
    "\n",
    "_________________\n",
    "\n",
    "Written by Laura Prichard, May 2021 (on GitHub [here](https://github.com/lprichard/HST_FLC_corrections)). Includes codes developed Ben Sunnquist (on [GitHub](https://github.com/bsunnquist/uvis-skydarks)) & Marc Rafelski. \n",
    "\n",
    "Please reference Prichard et al. 2021, *in prep.* (check back for updated reference) and codes by Ben Sunnquist if you use any of the corrections outlined here.\n",
    "\n",
    "Notebook tested with: `Python v3.7`, `astropy v4.0`, `astroquery v0.4`, `drizzlepac 3.1.8`, `photutils v1.0.2`, and `stwcs v1.6.1`. These versions and higher are recommended.\n",
    "_________________\n",
    "\n",
    "This notebook describes the step-by-step procedure to download data from the Mikulski Archive for Space Telescopes ([MAST](https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html)) with [`astroquery`](https://astroquery.readthedocs.io/en/latest/mast/mast.html) and apply additional calibrations to reduced Hubble Space Telescope (HST) single-visit images (called FLCs). Different corrections are needed for the Wide Field Camera 3 (WFC3)/UV-Visible (UVIS) and Advanced Camera for Surveys (ACS) FLCs that are listed below.  \n",
    "<br /> \n",
    "\n",
    "**WFC3/UVIS corrections**\n",
    "\n",
    "- As of May 2021, many improvements to the WFC3/UVIS darks pipeline (outlined on Laura Prichard's GitHub [here](https://github.com/lprichard/hst_wfc3_uvis_reduction)) have been adopted as standard by the WFC3 team. (These are detailed in Prichard et al. 2021.)\n",
    "- Therefore, much improved cleaned images are available on MAST that also include the new [time-dependent photometry](https://www.stsci.edu/hst/instrumentation/wfc3/data-analysis/photometric-calibration/uvis-photometric-calibration) information for WFC3/UVIS.\n",
    "- The additional corrections outlined here are to equalize the four amps of the WFC3/UVIS FLCs to reduce offsets (referenced as `medsub` in the image below). Correction code [`make_uvis_skydark.py`](https://github.com/bsunnquist/uvis-skydarks/blob/master/make_uvis_skydark.py) by Ben Sunnquist.\n",
    "- The other correction is to flag read out cosmic rays (ROCRs). ROCRs are a residual effect of the new and improved charge transfer efficiency (CTE) correction code (`calwf3 v3.6.0`; [Anderson 2020](https://ui.adsabs.harvard.edu/abs/2020wfc..rept....8A/abstract), [Anderson et al. 2021](https://www.stsci.edu/contents/news/wfc3-stans/wfc3-stan-issue-35-April-1.html#1%20-%20New%20CTE%20correction)). The new CTE code reduces gradients and reduces noise in the images. However, CRs that fall on the array while it's being read out are over corrected by the new CTE code as it does not know their real location. This results in divots in the combined images if these pixels are not flagged. Correction code [`flag_rocrs.ipynb`](https://github.com/bsunnquist/uvis-skydarks/blob/master/flag_rocrs.ipynb) by Ben Sunnquist. An example of a ROCR (dark tail of cosmic ray) is shown below.\n",
    "<img src=\"./images/ROCR.png\" alt=\"FLC\" width=\"200\"/>\n",
    "\n",
    "An example of the new corrections on the WFC3/UVIS FLCs (right panel) compared with the FLCs previously available on MAST (left) is shown below. The corrections shown in the image are the new CTE code, the improved darks (both now included as standard for WFC3/UVIS MAST FLCs), and the equalizing amps `make_uvis_skydark.py` (or `medsub`) routine.\n",
    "<img src=\"./images/FLC_comp.png\" alt=\"FLC\" width=\"400\"/>  \n",
    "<br />\n",
    "\n",
    "**ACS corrections**\n",
    "\n",
    "The corrections for ACS data are typically only required for a few exposures, and may affect those observed in Continuous Viewing Zones (CVZs) more.\n",
    "\n",
    "- Removing gradients caused by the reflection of the Sun on the Earthâ€™s atmosphere into the telescope at low limb angles (see [Biretta et al. 2003](https://ui.adsabs.harvard.edu/abs/2003acs..rept....5B/abstract) for more information). This is a stronger effect at the redder wavelengths covered by ACS. The image shows an example FLC with a strong gradient (left) and with the gradient removed (right). Correction code [`remove_gradients.ipynb`](https://github.com/bsunnquist/uvis-skydarks/blob/master/remove_gradients.ipynb) by Ben Sunnquist.\n",
    "<img src=\"./images/ACS_gradients.png\" alt=\"FLC\" width=\"400\"/>\n",
    "- NOTE: in future, the ACS FLCs may also need the ROCR correction as the CTE code for ACS will also be updated with similar improvements to the WFC3/UVIS CTE corrections. Check for any ACS CTE code updates [here](https://www.stsci.edu/hst/instrumentation/acs/performance/cte-information). The ROCR correction code is already written to handle both WFC3 and ACS FLCs.  \n",
    "<br />\n",
    "<br />\n",
    "\n",
    "**Extra FLC corrections if required**\n",
    "\n",
    "Ben Sunnquist also developed a routine to flag satellite trails or other anomalies in FLCs using DS9 region files. The code is not included below but the link to it with more details is here:\n",
    "[flag_regions.ipynb](https://github.com/bsunnquist/uvis-skydarks/blob/master/flag_regions.ipynb)\n",
    "\n",
    "The regions are flagged in the data quality (DQ) arrays and are then not included in the final drizzles. The image shows an example of the SCI extension (left) with DS9 region (green) and flagged DQ extension (right) of an FLC.\n",
    "<img src=\"./images/bsunnquist_region_flag.png\" alt=\"FLC\" width=\"400\"/>  \n",
    "<br />\n",
    "_________________\n",
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-composer",
   "metadata": {},
   "source": [
    "**Load packages and adjust display settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "from astropy.time import Time\n",
    "from astropy.modeling import models, fitting\n",
    "from astropy.convolution import Gaussian2DKernel\n",
    "from astropy.stats import sigma_clip, gaussian_fwhm_to_sigma, SigmaClip\n",
    "from astroquery.mast import Observations\n",
    "from stwcs import updatewcs\n",
    "from stsci.tools import teal\n",
    "from platform import python_version\n",
    "from drizzlepac import astrodrizzle as ad\n",
    "from photutils import Background2D, detect_sources, detect_threshold, MedianBackground\n",
    "\n",
    "# Load LP's codes in this directory\n",
    "import copy_files as cf\n",
    "\n",
    "# Setting pandas display options\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-document",
   "metadata": {},
   "source": [
    "# 1) Download & Organize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-episode",
   "metadata": {},
   "source": [
    "**a) <span style=\"color:red\">REQUIRED USER INPUTS:</span> Set your proposal ID, directories and options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# INPUTS\n",
    "\n",
    "# REQUIRED: Proposal ID\n",
    "PID = '12345'\n",
    "\n",
    "# REQUIRED: Root directory for FLCs, will be created if it doesn't exist\n",
    "FLC_DIR = 'your_root_data_directory/flcs_PID{}'.format(PID)   # Set proposal ID above\n",
    "\n",
    "# REQUIRED: Root path to the directory containing the downloaded codes\n",
    "CODE_DIR = 'your_code_directory'\n",
    "\n",
    "# REQUIRED: Set instrument for FLCs to download (either WFC3 or ACS), if running corrections for both ACS & WFC3 FLCs,\n",
    "# run this whole section (1) for each instrument to create seperate directories\n",
    "inst = 'WFC3' #'ACS' \n",
    "\n",
    "# Set to False to see the files to be downloaded, True to download the files\n",
    "download=True\n",
    "\n",
    "# If the files are downloaded (previously or with download=True), set copy=True (recommended) to copy each FLC\n",
    "# out of it's own subdirectory in the download directory (DLD_DIR set below) to a combined directory ALL_DIR\n",
    "copy=True\n",
    "\n",
    "# NOTE: if copy=True and download=False, manually set DLD_DIR to that of the astroquery download directory \n",
    "# (includes PID and download date) \n",
    "# DLD_DIR = ''    # EXAMPLE: os.path.join(FLC_DIR, 'mastDownload_PID12345_ACS_2021May18', 'HST') \n",
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sub-directories to be made\n",
    "ALL_DIR = os.path.join(FLC_DIR, 'all_flcs_{}'.format(inst.lower()))  # Directory for all FLCs combined\n",
    "COR_DIR = os.path.join(FLC_DIR, 'cor_flcs_{}'.format(inst.lower()))  # Directory for correcting FLCs\n",
    "    \n",
    "# Make the download data directory if it doesn't exist\n",
    "if os.path.exists(ALL_DIR):\n",
    "    print('ALL_DIR directory exists: {}'.format(ALL_DIR))\n",
    "else:\n",
    "    os.makedirs(ALL_DIR, 0o774)\n",
    "    print('Created ALL_DIR directory: {}'.format(ALL_DIR))\n",
    "    \n",
    "# Make the corrected data directory if it doesn't exist\n",
    "if os.path.exists(COR_DIR):\n",
    "    print('COR_DIR directory exists: {}'.format(COR_DIR))\n",
    "else:\n",
    "    os.makedirs(COR_DIR, 0o774)\n",
    "    print('Created COR_DIR directory: {}'.format(COR_DIR))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-republic",
   "metadata": {},
   "source": [
    "**b) Check available data products on MAST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-jersey",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search for science observations by proposal ID\n",
    "\n",
    "# Convert date string to MJD\n",
    "def mjd_to_str(t):\n",
    "    \"\"\"Converts Modified Julien Date (MJD) input (t) \n",
    "    to a readable date string (t_str).\"\"\"\n",
    "    t_mjd = Time(t, format='mjd')\n",
    "    t_str = t_mjd.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return t_str\n",
    "\n",
    "# Select all science observations for the proposal ID \n",
    "sciobs = Observations.query_criteria(intentType='science', proposal_id='{}'.format(PID))\n",
    "# See available columns in the result\n",
    "print(sciobs.columns)\n",
    "\n",
    "# Convert astropy table to a dataframe for manipulation\n",
    "so_df = pd.DataFrame(np.array(sciobs))\n",
    "\n",
    "# Get the easily readable date string from the MJD dates\n",
    "so_df['t_min_str'] = so_df['t_min'].apply(mjd_to_str)\n",
    "so_df['t_max_str'] = so_df['t_max'].apply(mjd_to_str)\n",
    "\n",
    "# Print just the ASN numbers (obs_id), start and end times (t_min, t_max) in MJD and string form in descending date order\n",
    "so_df[['obs_id', 'target_name', 'instrument_name', 't_min','t_min_str','t_max','t_max_str']].sort_values('t_min', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-simple",
   "metadata": {},
   "source": [
    "**c) Download the data and organize**\n",
    "\n",
    "If there are duplicate FLCs showing in the `mastflcs` list, `astroquery` download has caching so it won't actually download more than one of the FLCs with the same name. FLCs can be listed under different observation IDs (`parent_obsid`) so may listed more than once in `astroquery` but not on MAST that does not filter by that information.\n",
    "\n",
    "Check print command to see if the WFC3/UVIS data is processed with the new calibration codes (`calwf3 v3.6.0(Dec-31-2020)` and above) prior to downloading (set `download=False` above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_dload_data(DLD_DIR, ALL_DIR, ext='_flc.fits'):\n",
    "    \"\"\"Copies the downloaded files out of the nested astroquery directories \n",
    "    and into one combined directory (raw downloaded data, RWD_DIR). Here, \n",
    "    the CTE correction code copies over only the darks with the right exposure \n",
    "    times to be CTE corrected. Or calwf3 is run on all the raw science data files.\n",
    "    \"\"\"\n",
    "    # Move to the download directory\n",
    "    os.chdir(DLD_DIR)\n",
    "\n",
    "    # Copies data out of the astroquery sub-directories and into the RWD_DIR directory if empty\n",
    "    print('Copying downloaded raws from astroquery subdirectories {}'.format(DLD_DIR))\n",
    "    n=0\n",
    "    \n",
    "    for i, idob in enumerate(glob.glob('*')):\n",
    "        # Check if the file is already in the directory, if not it is copied\n",
    "        src = os.path.join(DLD_DIR, idob, idob +ext)\n",
    "        dst = os.path.join(ALL_DIR, idob +ext)\n",
    "        if not os.path.exists(dst):\n",
    "            print('Copying {} to {}'.format(src, dst))\n",
    "            shutil.copy(src, dst)\n",
    "            n+=1\n",
    "        else: \n",
    "            print('File exists {}, not copying file from {}'.format(dst, DLD_DIR))\n",
    "\n",
    "    print('Copied {} files to combined raw data directory {}'.format(n, ALL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-characteristic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search for files with astroquery by instrument and PID (specfied in section 1a) \n",
    "if inst=='WFC3': sciobs = Observations.query_criteria(intentType='science', instrument_name=\"WFC3/UVIS\", proposal_id='{}'.format(PID)) \n",
    "elif inst=='ACS': sciobs = Observations.query_criteria(intentType='science', instrument_name=\"ACS/WFC\", proposal_id='{}'.format(PID))\n",
    "\n",
    "# Get the FLCs for each instrument\n",
    "sciprod = Observations.get_product_list(sciobs)\n",
    "mastflcs = Observations.filter_products(sciprod, productSubGroupDescription=\"FLC\", type='S')\n",
    "print('{} FLCs found:'.format(len(mastflcs)))\n",
    "mastflcs['productFilename', 'project', 'prvversion'].pprint(max_lines=100)\n",
    "\n",
    "# If download option is set, check if the download directory exists, if not then download\n",
    "MDLD_DIR = os.path.join(FLC_DIR, 'mastDownload', 'HST')\n",
    "if download==True:\n",
    "    if os.path.exists(MDLD_DIR):\n",
    "        print('Download directory exists! Not downloading files')\n",
    "    else: \n",
    "        # Move to FLC directory and download FLCs with astroquery\n",
    "        os.chdir(FLC_DIR)\n",
    "        Observations.download_products(mastflcs, mrp_only=False) \n",
    "        \n",
    "        # Rename the download directory to add the PID and date\n",
    "        now = Time.now()\n",
    "        pnow = now.strftime('%Y%b%d')\n",
    "        DLD_DIR = MDLD_DIR.replace('mastDownload', 'mastDownload_PID{}_{}_{}'.format(PID, inst, pnow))\n",
    "        os.rename(os.path.dirname(MDLD_DIR), os.path.dirname(DLD_DIR))\n",
    "        print('Download to {} complete'.format(DLD_DIR))\n",
    "        \n",
    "# Copy files out of the sub-directories into a combined directory\n",
    "if copy==True:\n",
    "    if \"DLD_DIR\" in locals(): copy_dload_data(DLD_DIR, ALL_DIR, ext='_flc.fits')\n",
    "    else: print('WARNING: Download directory (DLD_DIR) not set, set download=True or manually set in Step 1a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-cheese",
   "metadata": {},
   "source": [
    "**d) Optional data check** \n",
    "\n",
    "Check if the WFC3/UVIS data is processed with the new calibration codes (`calwf3 v3.6.0(Dec-31-2020)` and above) and have time-dependent photometry info in their headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into the update WCS directory\n",
    "os.chdir(ALL_DIR)\n",
    "\n",
    "# List flcs\n",
    "files = sorted(glob.glob('*_flc.fits'))\n",
    "\n",
    "# Get FLC and processing information\n",
    "for f in files:\n",
    "    print('--------------------------------------------------------------')\n",
    "    # Open file header\n",
    "    hdr = fits.getheader(f, 0)\n",
    "    \n",
    "    # Get FLC info\n",
    "    print('FILE: {}, INST: {}, DATE OBS:{}'.format(f, hdr['INSTRUME'], hdr['DATE-OBS']))\n",
    "\n",
    "    # Get processing info\n",
    "    if 'WFC3' in hdr['INSTRUME']: caltype='CALWF3'\n",
    "    elif 'ACS' in hdr['INSTRUME']: caltype='CALACS'\n",
    "    print('DATE PROCESSED: {}, {} VERSION: {}'.format(hdr['DATE'], caltype, hdr['CAL_VER']))\n",
    "    \n",
    "    # Check for time-dependent phototmetry header updates in WFC3/UVIS files only\n",
    "    if '2020 Time-dependent Inverse Sensitivity' in str(hdr): timedep='True'\n",
    "    else: timedep='False'\n",
    "    if 'WFC3' in hdr['INSTRUME']: print('Includes new WFC3/UVIS time-dependent photometry: {}'.format(timedep))\n",
    "    print('--------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-cement",
   "metadata": {},
   "source": [
    "# 2) Apply WFC3/UVIS FLC Corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-indian",
   "metadata": {},
   "source": [
    "**a) Equalize amp offsets**\n",
    "\n",
    "This step equalizes the amplifiers (four quadrants) on WFC3/UVIS FLCs. The default behavior of this code measures the median of each amp, multiplies it by the flat, subtracts that from each amp, and equalizes the amps to the average amp value. This removes bias offsets between the quadrants to produce smoother images. The corrected output FLCs are the `*_flc_eq.fits` files.\n",
    "\n",
    "The code [`make_uvis_skydark.py`](https://github.com/bsunnquist/uvis-skydarks/blob/master/make_uvis_skydark.py) used for this step was developed by Ben Sunnquist. A copy of the code, downloaded from GitHub 17 May 2021 (last updated  Apr 19, 2021), is included in this directory but check the link to Ben's GitHub code for the latest version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "if inst=='WFC3':\n",
    "    # Set correction subdirectory \n",
    "    EQ_DIR = os.path.join(COR_DIR, 'eq_flcs')\n",
    "\n",
    "    # Directory is made and files copied over to be corrected\n",
    "    cf.copy_files_check(ALL_DIR, EQ_DIR, files='*_flc.fits')\n",
    "\n",
    "    # Copy over the make_uvis_skydark.py code from the code directory to the FLC correction directory\n",
    "    cf.copy_files_check(CODE_DIR, EQ_DIR, files='make_uvis_skydark.py')\n",
    "\n",
    "    # Move into the correction directory\n",
    "    os.chdir(EQ_DIR)\n",
    "\n",
    "    # Run the amp offset code\n",
    "    %run make_uvis_skydark.py\n",
    "else: print('Set inst=WFC3 and ensure you have the WFC3 FLCs to apply this correction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-bibliography",
   "metadata": {},
   "source": [
    "**b) Correct for read out cosmic rays (ROCRs)**\n",
    "\n",
    "Applying the ROCR correction to FLCs is a multi-stage process that changes more information than necessary on the FLCs. Therefore, multiple copies of the FLCs are made to ensure that the only thing changed on the final clean output FLCs is an updated data quality (DQ) array. \n",
    "\n",
    "The processed FLCs are ran through a WCS update (with `updatewcs`) of the header so that they can be run through `astrodrizzle` grouped by association number. This is done to create cosmic ray maps/flags used in the ROCR correction. The flagging of ROCR pixels is then performed on the DQ arrays of those FLCs processed with `astrodrizzle`. These DQ arrays are then copied back into the untouched clean copy of the FLCs.  \n",
    "<br />\n",
    "\n",
    "i) Copy and rename files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-positive",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set two new directories for updating WCS and for the final clean ROCR corrected FLCs\n",
    "WCS_DIR = os.path.join(COR_DIR, 'wcs_updt')      # For the FLCs to be processed\n",
    "ROCR_DIR = os.path.join(COR_DIR, 'rocr_clean')   # For the final clean FLCs that will have updated DQ arrays\n",
    "\n",
    "# Creates directories, copies, and renames files if they don't exist\n",
    "cf.copy_files_check(EQ_DIR, WCS_DIR, files='*flc_eq.fits', rename=True, src_str='flc_eq', dst_str='flc')\n",
    "cf.copy_files_check(EQ_DIR, ROCR_DIR, files='*flc_eq.fits', rename=True, src_str='flc_eq', dst_str='flc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-substitute",
   "metadata": {},
   "source": [
    "ii) Update WCS (to avoid `astrodrizzle` errors) and move the updated files to a drizzle directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into the update WCS directory\n",
    "os.chdir(WCS_DIR)\n",
    "\n",
    "# List flcs\n",
    "files = sorted(glob.glob('*_flc.fits'))\n",
    "\n",
    "# Update WCS in header to avoid errors with astrodrizzle\n",
    "updatewcs.updatewcs(files, use_db=False)    #use_db=False for use w drizzlepac 3.1.6 and above\n",
    "\n",
    "# Set the drizzle directory\n",
    "DRIZ_DIR = os.path.join(COR_DIR, 'rocr_driz')\n",
    "\n",
    "# Makes destination directory if it doesn't exist, checks if files exist, copies them if not\n",
    "cf.copy_files_check(WCS_DIR, DRIZ_DIR, files='*flc.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-length",
   "metadata": {},
   "source": [
    "iii) Get batches of FLC based on association (ASN i.e. observing group) number and run them through `astrodrizzle`\n",
    "\n",
    "`astrodrizzle` creates cosmic ray maps in the DQ arrays of each FLC. The drizzles themselves are not used but the FLCs that are edited by `astrodrizzle` are. Only the basic parameters with CR flagging are used for the drizzle.\n",
    "\n",
    "NOTE: The `driz_cr_snr` should be set to best suit your data. Tips from Ben Sunnquist: \"[The ROCR correction code (step v)] typically flagged an additional ~ 5000-50,000 pixels in each chip [(this is printed out in step v)]. If you find it's over/under flagging... you could raise/lower the sigma in the ROCR code [(step e)], or set the `driz_cr_snr='5 4'` [(or higher, below in this drizzle step)] rather than `'3.5 3'` when making the cosmic ray maps, which sometimes over flags CRs (and thus can over flag ROCRs as well). To verify, I blink the FLC SCI (ext=1 & 4) and DQ (ext=3 & 6) extensions, and make sure the negative tails attached to some CRs are flagged.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into the drizzle directory\n",
    "os.chdir(DRIZ_DIR)\n",
    "\n",
    "# Get all FLCs\n",
    "files = sorted(glob.glob('*_flc.fits'))\n",
    "\n",
    "# Loop to get batches of all files to run through astrodrizzle based on ASN ID\n",
    "fields = []\n",
    "asns_full = []\n",
    "asns = []\n",
    "for f in files:\n",
    "\n",
    "    #Read in header for each file and get field and ASN ID\n",
    "    h = fits.open(f)\n",
    "    field = h[0].header['TARGNAME']\n",
    "    asn = h[0].header['ASN_ID']\n",
    "\n",
    "    # For each ASN ID, store the full ASN ID, file abreviation, and fields/target names of observations\n",
    "    if asn not in asns_full:\n",
    "        asns_full.append(asn)\n",
    "        asns.append(f[0:6])\n",
    "        fields.append(field)  \n",
    "\n",
    "print('Unique ASNs: {}'.format(asns_full))\n",
    "print('Unique ASN filenames: {}'.format(asns))\n",
    "print('Fields: {}'.format(fields))\n",
    "\n",
    "# Create lists of files associated with each ASN ID:\n",
    "lists = []\n",
    "for asn in asns:\n",
    "    asn_files = [files[i] for i, s in enumerate(files) if asn in s]\n",
    "    lists.append(asn_files)\n",
    "\n",
    "print(' Lists of files for each ASN ID that will be ran by astrodrizzle in batches:')\n",
    "print(lists)\n",
    "\n",
    "# Get versions\n",
    "teal.unlearn('astrodrizzle')\n",
    "print('Python version {}'.format(python_version()))\n",
    "ad.__version__\n",
    "\n",
    "# Timestamp for drizzles\n",
    "now = datetime.datetime.now()\n",
    "print('*****************************************************************************')\n",
    "print(DRIZ_DIR)\n",
    "print('Drizzle started at ', now.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "print('*****************************************************************************\\n\\n')\n",
    "\n",
    "# Run astrodrizzle with lists and ASN IDs defined above\n",
    "for l, asn in zip(lists, asns):\n",
    "    ad.AstroDrizzle(l, \n",
    "        driz_cr_corr=True, \n",
    "        driz_combine=True,\n",
    "        preserve=False,  \n",
    "        clean=True, \n",
    "        build=True, \n",
    "        driz_cr_snr='3.5 3.0'    # Set this option to best suit your data ('5.0 4.0' or higher), see notes above\n",
    "        output='{}'.format(asn))\n",
    "\n",
    "# Timestamp for drizzles\n",
    "now = datetime.datetime.now()\n",
    "print('\\n\\n*****************************************************************************')\n",
    "print(DRIZ_DIR)\n",
    "print('Drizzle complete at ', now.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "print('*****************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-chair",
   "metadata": {},
   "source": [
    "iv) If desired, make an additional copy of the drizzled, but not yet ROCR corrected, FLCs\n",
    "\n",
    "The drizzle can take a while depending on your data set. If you would like to test the ROCR flagging parameters on a clean set of drizzled FLCs each time, then make an extra copy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes the directory if it doesn't exist, checks for files, copies them over if not there\n",
    "cf.copy_files_check(DRIZ_DIR, DRIZ_DIR.replace('rocr_driz', 'PRErocr_driz'), files='*flc.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-acoustic",
   "metadata": {},
   "source": [
    "v) Run the ROCR corrections\n",
    "\n",
    "This code was developed by Ben Sunnquist and the original version is here: [flag_rocrs.ipynb](https://github.com/bsunnquist/uvis-skydarks/blob/master/flag_rocrs.ipynb). Check there for the latest version. \n",
    "\n",
    "I made some small edits (denoted with LP in the comments) so that it would be compatible with `Python v3.7`/`astropy v4.0` (the original works with `Python v3.6` and `astropy v<4.0`).\n",
    "\n",
    "Tips from Ben Sunnquist: \"[The ROCR correction code (this step)] typically flagged an additional ~ 5000-50,000 pixels in each chip [(this is printed out)].\" Adjust the threshold parameter below or adjust `driz_cr_snr` in step iii to get this level of flagging. To check outputs: \"blink the FLC SCI (ext=1 & 4) and DQ (ext=3 & 6) extensions, and make sure the negative tails attached to some CRs are flagged.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag pixels as \"bad detector pixel\" (DQ value 4) that are within 5 pixels of a CR hit (away from the\n",
    "# readout direction) AND X sigma below the image mean (where the sigma and mean here are from a Gaussian \n",
    "# fit to the sigma-clipped image data).\n",
    "#\n",
    "# These pixels are read out cosmic rays (ROCRs), i.e. CRs that fall during readout and therefore \n",
    "# trick the CTE correction into over correcting them since it thinks they fell farther from the readout\n",
    "# than they actually did.\n",
    "\n",
    "\n",
    "###################################### USER INPUTS ######################################\n",
    "# LP added: Move into the drizzle directory\n",
    "os.chdir(DRIZ_DIR)\n",
    "\n",
    "# the files to use to find the ROCRs (i.e. drizzling has been done on these files so they have CR flags)\n",
    "files = sorted(glob.glob('*_flc.fits'))  # the files to flag ROCRs in\n",
    "\n",
    "# the directory containing the files to add the ROCR flags to (no drizzling has been done on these files)\n",
    "# LP edit: set to the pre-made clean FLC directory\n",
    "untouched_files_dir = ROCR_DIR\n",
    "\n",
    "# the sigma to use when determining the threshold for flagging ROCRs\n",
    "sigma = 2.75   # LP note: adjust to flag the appropriate no. of ROCR pixels for your data\n",
    "#########################################################################################\n",
    "\n",
    "for f in files:\n",
    "    basename = os.path.basename(f)\n",
    "    print('Flagging ROCRs in {} ...'.format(basename))\n",
    "    untouched_file = os.path.join(untouched_files_dir, basename)\n",
    "    h = fits.open(f)\n",
    "    h_untouched = fits.open(untouched_file)\n",
    "\n",
    "    for ext in [1,4]:\n",
    "        data = h[ext].data\n",
    "        dq = h[ext+2].data\n",
    "        dq_untouched = h_untouched[ext+2].data\n",
    "\n",
    "        # Find lower limit for flaggincg ROCRs\n",
    "        clipped = sigma_clip(data, sigma=3, maxiters=5)\n",
    "        d = clipped[clipped.mask==False].data\n",
    "        n, bins = np.histogram(d, bins=70)\n",
    "        bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "        #LP added [0:1] indexes\n",
    "        g_init = models.Gaussian1D(amplitude=np.array(n[n==max(n)][0:1]), mean=np.array(bin_centers[n==max(n)][0:1]), stddev=np.std(d))\n",
    "        \n",
    "        fit_g = fitting.LevMarLSQFitter()\n",
    "        g = fit_g(g_init, bin_centers, n)\n",
    "        # LP removed [0] from g.mean.value\n",
    "        thresh = g.mean.value - sigma*g.stddev.value\n",
    "        print('\\t Threshold Ext {} = {:.3f} - {}*{:.3f} = {:.3f}'.format(ext, g.mean.value, sigma, g.stddev.value, thresh)) \n",
    "        \n",
    "        # Make mask of all CR hits\n",
    "        cr_mask = np.zeros(dq.shape, dtype=int)\n",
    "        cr_mask[dq&4096!=0] = 1\n",
    "\n",
    "        # Flag pixels within 5 pixels of a CR hit (away from readout) that are below the threshold \n",
    "        coords = np.where(cr_mask==1)\n",
    "        cr_mask_new = np.zeros(cr_mask.shape)\n",
    "        for i in np.arange(len(coords[0])):\n",
    "            x,y = coords[1][i], coords[0][i]\n",
    "\n",
    "            # Get the first y-coordinate to check\n",
    "            if ext==1:\n",
    "                running_y = y + 1\n",
    "            elif ext==4:\n",
    "                running_y = y - 1\n",
    "            else:\n",
    "                print('extension {} not expected'.format(ext))\n",
    "\n",
    "            # See if this coordinate has a value below the threshold\n",
    "            count = 0\n",
    "            while count < 5:  # stay within 5 pixels of cr hit\n",
    "                if (running_y <= 2050) & (running_y >= 0):  # avoid going off the image y-dimension          \n",
    "                    val = data[running_y, x]\n",
    "                    if val < thresh:\n",
    "                        cr_mask_new[running_y, x] = 1\n",
    "                    if ext==1:\n",
    "                        running_y += 1\n",
    "                    elif ext==4:\n",
    "                        running_y -= 1\n",
    "                    else:\n",
    "                        print('extension {} not expected'.format(ext))\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                count += 1\n",
    "\n",
    "        # Add in new ROCR flags as 4 (bad detector pixel)\n",
    "        dq_untouched[(dq_untouched&4==0) & (cr_mask_new==1)] += 4\n",
    "        h_untouched[ext+2].data = dq_untouched\n",
    "\n",
    "        # Write out ROCR flag map\n",
    "        fits.writeto(f.replace('_flc.fits','_rocr_map_ext_{}.fits'.format(ext)), cr_mask_new, overwrite=True)\n",
    "        print('\\t # of ROCR flags in Ext {}: {}'.format(ext, len(cr_mask_new[cr_mask_new==1])))\n",
    "\n",
    "    # Write out the ROCR-flagged flc file\n",
    "    outname = untouched_file.replace('_flc.fits','_rocr_flagged_flc.fits')\n",
    "    h_untouched.writeto(outname, overwrite=False)\n",
    "    h_untouched.close()\n",
    "    print('\\t ROCR-flagged image saved to {}'.format(os.path.basename(outname)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-floor",
   "metadata": {},
   "source": [
    "vi) Set a final directory, copy and rename FLCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set final direcotry\n",
    "FIN_DIR = os.path.join(FLC_DIR, 'final_flcs')\n",
    "\n",
    "# Makes destination directory, checks if files exist, copies them if they don't, and renames files as specified\n",
    "cf.copy_files_check(ROCR_DIR, FIN_DIR, files='*_rocr_flagged_flc.fits', rename=True, src_str='rocr_flagged_flc', dst_str='flc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-armor",
   "metadata": {},
   "source": [
    "vii) Update headers of final corrected FLCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into directory\n",
    "os.chdir(FIN_DIR)\n",
    "\n",
    "# Set time now\n",
    "now = Time(Time.now(), format='iso')\n",
    "\n",
    "# List flcs\n",
    "flcs = glob.glob('*flc.fits')\n",
    "\n",
    "# update filenames, history, date\n",
    "n=0\n",
    "for f in flcs:\n",
    "    print('Opening and updating {}...'.format(f))\n",
    "\n",
    "    # Open file for editing\n",
    "    h = fits.open(f)\n",
    "    \n",
    "    # Update header with corrections performed\n",
    "    h[0].header['HISTORY'] = 'FLC corrections (from L Prichard) performed {}'.format(now)\n",
    "    h[0].header['HISTORY'] = 'Code: https://github.com/lprichard/HST_FLC_corrections.ipynb'\n",
    "    h[0].header['HISTORY'] = 'Includes FLC amp equalization (by B Sunnquist)'\n",
    "    h[0].header['HISTORY'] = 'Code: https://github.com/bsunnquist/uvis-skydarks/blob/master/make_uvis_skydark.py'\n",
    "    h[0].header['HISTORY'] = 'Includes ROCR corrections (by B Sunnquist)'\n",
    "    h[0].header['HISTORY'] = 'Code: https://github.com/bsunnquist/uvis-skydarks/blob/master/flag_rocrs.ipynb'\n",
    "\n",
    "    h.writeto(f, overwrite=True)\n",
    "    h.close()\n",
    "    n+=1\n",
    "    print('Updated header for {}/{}: {}'.format(n, len(flcs), f))\n",
    "\n",
    "print('Updated headers for {} final flcs in {}'.format(n, FIN_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-angle",
   "metadata": {},
   "source": [
    "# 2) Apply ACS FLC Corrections\n",
    "\n",
    "**a) Gradient removal and chip equalization for ACS FLCs**\n",
    "\n",
    "Gradients may only exist in a handful of FLCs and may be more likely to have strong gradients if observed in [Continuous Viewing Zones (CZVs)](https://www.stsci.edu/itt/review/cp_primer_cy17/CP_PRIMER/4_Observation_Types2.html). Correction code [`remove_gradients.ipynb`](https://github.com/bsunnquist/uvis-skydarks/blob/master/remove_gradients.ipynb) by Ben Sunnquist, check for latest version. By default, the code only applies corrections to FLCs with gradients larger than 5 electrons (`gradient_threshold=5`). To apply the corrections (gradient removal and chip equalization) to all FLCs (which results in basically the same output for those FLCs least affected), set `gradient_threshold=None`.\n",
    "\n",
    "Examples of the outputs of the code are below:\n",
    "<img src=\"./images/remove_grad_outputs.png\" alt=\"grad\" width=\"400\"/>\n",
    "\n",
    "Notes from Ben Sunnquist on the code functionality and tips for its application:\n",
    "\n",
    "\"The following is the full process used to remove the gradients from the input FLCs:\n",
    "1. Subtract the clipped median to remove the overall background level\n",
    "2. Find the 2D background gradient of #1 using `photutils` 2D median background estimator\n",
    "3. Subtract the gradient found in #2 from the original FLC data to remove the gradient\n",
    "4. Create a source segmap using the image from #3\n",
    "5. Repeat steps 1-3 using the original, untouched FLCs, but this time masking the sources found in #4 when finding the background gradient\n",
    "6. [Equalizing the gradient subtracted chips to the average chip level as for the WFC3/UVIS FLCs]\n",
    "\n",
    "Documentation on the `photutils` background estimator [here](https://photutils.readthedocs.io/en/stable/background.html#d-background-and-noise-estimation) and on the `photutils` image segmentation for source finding [here](https://photutils.readthedocs.io/en/stable/segmentation.html#source-extraction-using-image-segmentation).\n",
    "\n",
    "For each FLC, the code will output a corresponding `*bkg.fits` and `*segmap.fits` image to inspect the quality of the background gradient subtracted and the source finding. If you find remnants of e.g. diffuse, patchy sources showing in the `*bkg.fits` image, I would recommend either increasing the box size in the background estimate, or decreasing the `nsigma` threshold used in the source detection.\"  \n",
    "<br />\n",
    "\n",
    "i) Copy the FLCs to be corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-angola",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if inst=='ACS':\n",
    "    # Set correction subdirectory \n",
    "    EQ_DIR = os.path.join(COR_DIR, 'eq_flcs')\n",
    "\n",
    "    # Directory is made and files copied over (if they don't exist) to be corrected\n",
    "    cf.copy_files_check(ALL_DIR, EQ_DIR, files='*_flc.fits')\n",
    "else: print('Set inst=ACS and ensure you have the ACS FLCs to apply this correction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-librarian",
   "metadata": {},
   "source": [
    "ii) Remove gradients from FLCs and equalize chip levels\n",
    "\n",
    "(Only some minor edits have been made to the original code, denoted by LP in the comments.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes large-scale background gradients from the input flc files, and equalizes the overall\n",
    "# background levels between chips\n",
    "\n",
    "\n",
    "################################# USER INPUTS #################################\n",
    "# LP added: Move into the correction sub-dir\n",
    "os.chdir(EQ_DIR)\n",
    "\n",
    "# The files to correct\n",
    "files = glob.glob('./*flc.fits')\n",
    "\n",
    "# Only those files whose gradients larger than this threshold will be corrected.\n",
    "# Set to None to correct all files regardless of the measured gradient.\n",
    "gradient_threshold = 5.0\n",
    "\n",
    "# The box size to use when creating the 2D background image\n",
    "box_size = (128, 128)\n",
    "\n",
    "# Option to mask sources when finding the background gradient/pedestal level\n",
    "mask_sources = True\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# Remove those files with no gradient from the processing list\n",
    "if gradient_threshold:\n",
    "    files_to_process = []\n",
    "    for f in files:\n",
    "        diffs = []\n",
    "        for ext in [1, 4]:\n",
    "            data = fits.getdata(f, ext)\n",
    "            left, _,  _, right = np.split(data, 4, axis=1)\n",
    "            clipped_left = sigma_clip(left, sigma=3, maxiters=5)\n",
    "            clipped_right = sigma_clip(right, sigma=3, maxiters=5)\n",
    "            med_left = np.nanmedian(clipped_left.data[clipped_left.mask==False])\n",
    "            med_right = np.nanmedian(clipped_right.data[clipped_right.mask==False])\n",
    "            diffs.append(abs(med_left - med_right))\n",
    "        diffs = np.array(diffs)\n",
    "        if len(diffs[diffs > gradient_threshold]) > 0:\n",
    "            files_to_process.append(f)\n",
    "else:\n",
    "    files_to_process = files\n",
    "\n",
    "# LP added files check\n",
    "if len(files_to_process)>0:\n",
    "    \n",
    "    # STEP 1: Remove the large-scale 2D background gradient from each chip\n",
    "    print('STEP 1: Removing 2D gradients from input FLCs...')\n",
    "    for f in files_to_process:\n",
    "        basename = os.path.basename(f)\n",
    "        print('Working on {}:'.format(basename))\n",
    "        h = fits.open(f)\n",
    "        for ext in [1,4]:\n",
    "            print('\\tWorking on extension {}:'.format(ext))\n",
    "            data_orig = np.copy(h[ext].data)\n",
    "            data = h[ext].data\n",
    "\n",
    "            # Subtract off median\n",
    "            clipped = sigma_clip(data, sigma=3, maxiters=5)\n",
    "            data = data - np.nanmedian(clipped.data[clipped.mask==False])\n",
    "\n",
    "            # Find sources in the gradient-removed image\n",
    "            if mask_sources:\n",
    "                print('\\tMaking source segmap...')\n",
    "                s = SigmaClip(sigma=3.)\n",
    "                bkg_estimator = MedianBackground()\n",
    "                bkg = Background2D(data, box_size=box_size, filter_size=(10, 10), \n",
    "                                   sigma_clip=s, bkg_estimator=bkg_estimator, exclude_percentile=15.0)\n",
    "                skydark = bkg.background\n",
    "                data_flat = data_orig - skydark\n",
    "                threshold = detect_threshold(data_flat, nsigma=0.75)\n",
    "                sigma = 3.0 * gaussian_fwhm_to_sigma\n",
    "                kernel = Gaussian2DKernel(sigma, x_size=3, y_size=3)\n",
    "                kernel.normalize()\n",
    "                segm = detect_sources(data_flat, threshold, npixels=5, filter_kernel=kernel)\n",
    "                segmap = segm.data\n",
    "                fits.writeto(f.replace('_flc.fits', '_segmap_ext{}.fits'.format(ext)), \n",
    "                             segmap, overwrite=True)\n",
    "            else:\n",
    "                segmap = np.zeros(data.shape).astype(int)\n",
    "\n",
    "            # Find the background gradient, incorporating the source mask\n",
    "            print('\\tFinding the background gradient...')\n",
    "            s = SigmaClip(sigma=3.)\n",
    "            bkg_estimator = MedianBackground()\n",
    "            mask = (segmap > 0)\n",
    "            bkg = Background2D(data, box_size=box_size, filter_size=(10, 10), \n",
    "                               sigma_clip=s, bkg_estimator=bkg_estimator, mask=mask, exclude_percentile=15.0)\n",
    "            skydark = bkg.background\n",
    "            fits.writeto(f.replace('_flc.fits', '_bkg_ext{}.fits'.format(ext)), \n",
    "                         skydark, overwrite=True)\n",
    "\n",
    "            # Subtract the background gradient from the original image\n",
    "            data_new = data_orig - skydark\n",
    "            h[ext].data = data_new.astype('float32')\n",
    "\n",
    "        h.writeto(f, overwrite=True)\n",
    "        h.close()\n",
    "        print('Finished removing background gradient from {}'.format(basename))\n",
    "       \n",
    "    \n",
    "    # STEP 2: equalize the overall background/pedestal level between chips to the average of the two chips\n",
    "    print('\\nSTEP 2: Equalizing overall background levels in the input FLCs...')\n",
    "    for f in files_to_process:\n",
    "        basename = os.path.basename(f)\n",
    "        print('Working on {}:'.format(basename))\n",
    "        h = fits.open(f)\n",
    "\n",
    "        # Find the overall background levels in each chip\n",
    "        background_levels = []\n",
    "        for ext in [1,4]:\n",
    "            data = np.copy(h[ext].data)\n",
    "\n",
    "            # Mask sources using the previous segmap\n",
    "            if mask_sources:\n",
    "                segmap = fits.getdata(f.replace('_flc.fits', '_segmap_ext{}.fits'.format(ext)))\n",
    "            else:\n",
    "                segmap = np.zeros(data.shape).astype(int)\n",
    "            data[segmap > 0] = np.nan\n",
    "\n",
    "            # Calculate the background level in the chip\n",
    "            clipped = sigma_clip(data, sigma=3, maxiters=5)\n",
    "            background_levels.append(np.nanmedian(clipped.data[clipped.mask==False]))\n",
    "            print('\\tBackground in ext {} = {:0.5f}'.format(ext, np.nanmedian(clipped.data[clipped.mask==False])))\n",
    "\n",
    "        # Equalize the background levels of the two chips to the average of the two\n",
    "        avg_bkg = np.mean(background_levels)\n",
    "        ext1_orig = np.copy(h[1].data)\n",
    "        ext1_diff = background_levels[0] - avg_bkg\n",
    "        ext1_new = ext1_orig - ext1_diff\n",
    "        h[1].data = ext1_new.astype('float32')\n",
    "        ext4_orig = np.copy(h[4].data)\n",
    "        ext4_diff = background_levels[1] - avg_bkg\n",
    "        ext4_new = ext4_orig - ext4_diff\n",
    "        h[4].data = ext4_new.astype('float32')\n",
    "\n",
    "        # Write out the final calibrated file\n",
    "        h.writeto(f, overwrite=True)\n",
    "        h.close()\n",
    "        print('Finished equalizing background levels between chips in {}'.format(basename))\n",
    "        \n",
    "# LP added\n",
    "else: print('{} files with gradient_threshold>{}, no gradients removed'.format(len(files_to_process), gradient_threshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-leonard",
   "metadata": {},
   "source": [
    "iii) Set a final directory and copy FLCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set final direcotry\n",
    "FIN_DIR = os.path.join(FLC_DIR, 'final_flcs')\n",
    "\n",
    "# Makes destination directory, checks if files exist, copies them if they don't, and renames files as specified\n",
    "cf.copy_files_check(ROCR_DIR, FIN_DIR, files='*flc.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-cannon",
   "metadata": {},
   "source": [
    "iv) Update headers of final corrected FLCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into directory\n",
    "os.chdir(FIN_DIR)\n",
    "\n",
    "# Set time now\n",
    "now = Time(Time.now(), format='iso')\n",
    "\n",
    "# List flcs\n",
    "flcs = glob.glob('*flc.fits')\n",
    "\n",
    "# update filenames, history, date\n",
    "n=0\n",
    "for f in flcs:\n",
    "    print('Opening and updating {}...'.format(f))\n",
    "\n",
    "    # Open file for editing\n",
    "    h = fits.open(f)\n",
    "    \n",
    "    # Update header with corrections performed\n",
    "    h[0].header['HISTORY'] = 'FLC corrections (from L Prichard) performed {}'.format(now)\n",
    "    h[0].header['HISTORY'] = 'Code: Code: https://github.com/lprichard/HST_FLC_corrections.ipynb'\n",
    "    h[0].header['HISTORY'] = 'Includes gradient removal & amp equalization (by B Sunnquist)'\n",
    "    h[0].header['HISTORY'] = 'Code: https://github.com/bsunnquist/uvis-skydarks/blob/master/remove_gradients.ipynb'\n",
    "    \n",
    "    h.writeto(f, overwrite=True)\n",
    "    h.close()\n",
    "    n+=1\n",
    "    print('Updated header for {}/{}: {}'.format(n, len(flcs), f))\n",
    "\n",
    "print('Updated headers for {} final flcs in {}'.format(n, FIN_DIR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
