{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unique-smooth",
   "metadata": {},
   "source": [
    "# Additional Corrections to HST FLCs\n",
    "\n",
    "_________________\n",
    "\n",
    "Written by Laura Prichard, May 2021. Includes codes developed Ben Sunnquist (on [GitHub](https://github.com/bsunnquist/uvis-skydarks)) & Marc Rafelski. \n",
    "\n",
    "Please reference Prichard et al. 2021, *in prep.* (check back for updated reference) and codes by Ben Sunnquist if you use any of the corrections outlined here.\n",
    "\n",
    "Notebook tested with: `Python v3.7`, `astropy v4.0`, `astroquery v0.4`, `drizzlepac 3.1.8`, `photutils v1.0.2`, and `stwcs v1.6.1`. These versions and higher are recommended.\n",
    "_________________\n",
    "\n",
    "This notebook describes the step-by-step procedure to download data from [MAST](https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html) with [`astroquery`](https://astroquery.readthedocs.io/en/latest/mast/mast.html) and apply additional calibrations to reduced Hubble Space Telescope (HST) single-visit images (called FLCs). Different corrections are needed for the Wide Field Camera 3 (WFC3)/UV-Visible (UVIS) and Advanced Camera for Surveys (ACS) FLCs that are listed below.  \n",
    "<br /> \n",
    "\n",
    "**WFC3/UVIS corrections**\n",
    "\n",
    "- As of May 2021, some improvements to the WFC3/UVIS darks pipeline (outlined on Laura Prichard's GitHub [here](https://github.com/lprichard/hst_wfc3_uvis_reduction)) have been adopted as standard by the WFC3 team. (These are detailed in Prichard et al. 2021, *in prep.*)\n",
    "- Therefore, much improved cleaned images are available on MAST that also include the new [time-dependent photometry](https://www.stsci.edu/hst/instrumentation/wfc3/data-analysis/photometric-calibration/uvis-photometric-calibration) information for WFC3/UVIS.\n",
    "- The first FLC correction is to apply a new hot pixel flagging method (developed by Laura Prichard) to more efficiently find hot pixels (~50-60% more). The previous method used a constant threshold for finding hot pixels (red lines) which missed increasing number of them further from the read out due to the inefficient transfer of charge. We instead derive a variable threshold to flag roughly the same number of hot pixels across the whole array (blue lines). \n",
    "<img src=\"./images/hotpix.png\" alt=\"FLC\" width=\"400\"/>  \n",
    "- The next correction is to equalize the four amps of the WFC3/UVIS FLCs to reduce offsets (referenced as `medsub` in the image below). Correction code [`make_uvis_skydark.py`](https://github.com/bsunnquist/uvis-skydarks/blob/master/make_uvis_skydark.py) by Ben Sunnquist.\n",
    "- The other correction is to flag read out cosmic rays (ROCRs). ROCRs are a residual effect of the new and improved charge transfer efficiency (CTE) correction code (`calwf3 v3.6.0`; [Anderson 2020](https://ui.adsabs.harvard.edu/abs/2020wfc..rept....8A/abstract), [Anderson et al. 2021](https://www.stsci.edu/contents/news/wfc3-stans/wfc3-stan-issue-35-April-1.html#1%20-%20New%20CTE%20correction)). The new CTE code reduces gradients and noise in the images. However, CRs that fall on the array while it's being read out are over corrected by the new CTE code as it does not know their real location. This results in divots in the combined images if these pixels are not flagged. Correction code [`flag_rocrs.ipynb`](https://github.com/bsunnquist/uvis-skydarks/blob/master/flag_rocrs.ipynb) by Ben Sunnquist. An example of a ROCR (dark tail of cosmic ray) is shown below.\n",
    "<img src=\"./images/ROCR.png\" alt=\"FLC\" width=\"200\"/>\n",
    "\n",
    "\n",
    "An example of the new corrections on the WFC3/UVIS FLCs (right panel) compared with the FLCs previously available on MAST (left) is shown below. The corrections shown in the image are the new CTE code, the improved darks (both now included as standard for WFC3/UVIS MAST FLCs), and the equalizing amps `make_uvis_skydark.py` (or `medsub`) routine.\n",
    "<img src=\"./images/FLC_comp.png\" alt=\"FLC\" width=\"400\"/>  \n",
    "<br />\n",
    "\n",
    "**ACS corrections**\n",
    "\n",
    "The corrections for ACS data are typically only required for a few exposures, and may affect those observed in Continuous Viewing Zones (CVZs) more.\n",
    "\n",
    "- Removing gradients caused by the reflection of the Sun on the Earthâ€™s atmosphere into the telescope at low limb angles (see [Biretta et al. 2003](https://ui.adsabs.harvard.edu/abs/2003acs..rept....5B/abstract) for more information). This is a stronger effect at the redder wavelengths covered by ACS. The image shows an example FLC with a strong gradient (left) and with the gradient removed (right). Correction code [`remove_gradients.ipynb`](https://github.com/bsunnquist/uvis-skydarks/blob/master/remove_gradients.ipynb) by Ben Sunnquist.\n",
    "<img src=\"./images/ACS_gradients.png\" alt=\"FLC\" width=\"400\"/>\n",
    "- NOTE: in future, the ACS FLCs may also need the hot pixel and ROCR correction as the CTE code for ACS will also be updated with similar improvements to the WFC3/UVIS CTE corrections. Check for any ACS CTE code updates [here](https://www.stsci.edu/hst/instrumentation/acs/performance/cte-information). The ROCR correction code is already written to handle both WFC3 and ACS FLCs.  \n",
    "<br />\n",
    "<br />\n",
    "\n",
    "**Extra FLC corrections if required**\n",
    "\n",
    "Ben Sunnquist also developed a routine to flag satellite trails or other anomalies in FLCs using DS9 region files. The code is not included below but the link to it with more details is here:\n",
    "[`flag_regions.ipynb`](https://github.com/bsunnquist/uvis-skydarks/blob/master/flag_regions.ipynb)\n",
    "\n",
    "The regions are flagged in the data quality (DQ) arrays and are then not included in the final drizzles. The image shows an example of the SCI extension (left) with DS9 region (green) and flagged DQ extension (right) of an FLC.\n",
    "<img src=\"./images/bsunnquist_region_flag.png\" alt=\"FLC\" width=\"400\"/>  \n",
    "<br />\n",
    "_________________\n",
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-composer",
   "metadata": {},
   "source": [
    "**Load packages and adjust display settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.time import Time\n",
    "from astropy.modeling import models, fitting\n",
    "from astropy.convolution import Gaussian2DKernel\n",
    "from astropy.stats import sigma_clip, gaussian_fwhm_to_sigma, SigmaClip\n",
    "from astroquery.mast import Observations\n",
    "from stwcs import updatewcs\n",
    "from stsci.tools import teal\n",
    "from crds.sync import SyncScript\n",
    "from platform import python_version\n",
    "from drizzlepac import astrodrizzle as ad\n",
    "from photutils import Background2D, detect_sources, detect_threshold, MedianBackground\n",
    "\n",
    "# Load LP's codes in this directory\n",
    "import copy_files as cf\n",
    "\n",
    "# Setting pandas display options\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Smoothing function\n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-document",
   "metadata": {},
   "source": [
    "# 1) Download & Organize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-episode",
   "metadata": {},
   "source": [
    "**a) <span style=\"color:red\">REQUIRED USER INPUTS:</span> Set your proposal ID, directories and options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# INPUTS\n",
    "\n",
    "# REQUIRED: Proposal ID\n",
    "PID = '12345'\n",
    "\n",
    "# REQUIRED: Root directory for FLCs, will be created if it doesn't exist\n",
    "FLC_DIR = 'your_root_data_directory/flcs_PID{}'.format(PID)   # Set proposal ID above\n",
    "\n",
    "# REQUIRED: Root path to the directory containing the downloaded codes\n",
    "CODE_DIR = 'your_code_directory'\n",
    "\n",
    "# REQUIRED: Set instrument for FLCs to download (either WFC3 or ACS), if running corrections for both ACS & WFC3 FLCs,\n",
    "# run this whole section (1) for each instrument to create seperate directories\n",
    "inst = 'WFC3' #'ACS' \n",
    "\n",
    "# Set to False to see the files to be downloaded, True to download the files\n",
    "download=True\n",
    "\n",
    "# If the files are downloaded (previously or with download=True), set copy=True (recommended) to copy each FLC\n",
    "# out of it's own subdirectory in the download directory (DLD_DIR set below) to a combined directory ALL_DIR\n",
    "copy=True\n",
    "\n",
    "# NOTE: if copy=True and download=False, manually set DLD_DIR to that of the astroquery download directory \n",
    "# (includes PID and download date) \n",
    "# DLD_DIR = ''    # EXAMPLE: os.path.join(FLC_DIR, 'mastDownload_PID12345_ACS_2021May18', 'HST') \n",
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sub-directories to be made\n",
    "ALL_DIR = os.path.join(FLC_DIR, 'all_flcs_{}'.format(inst.lower()))  # Directory for all FLCs combined\n",
    "COR_DIR = os.path.join(FLC_DIR, 'cor_flcs_{}'.format(inst.lower()))  # Directory for correcting FLCs\n",
    "    \n",
    "# Make the download data directory if it doesn't exist\n",
    "if os.path.exists(ALL_DIR):\n",
    "    print('ALL_DIR directory exists: {}'.format(ALL_DIR))\n",
    "else:\n",
    "    os.makedirs(ALL_DIR, 0o774)\n",
    "    print('Created ALL_DIR directory: {}'.format(ALL_DIR))\n",
    "    \n",
    "# Make the corrected data directory if it doesn't exist\n",
    "if os.path.exists(COR_DIR):\n",
    "    print('COR_DIR directory exists: {}'.format(COR_DIR))\n",
    "else:\n",
    "    os.makedirs(COR_DIR, 0o774)\n",
    "    print('Created COR_DIR directory: {}'.format(COR_DIR))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-republic",
   "metadata": {},
   "source": [
    "**b) Check available data products on MAST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-jersey",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search for science observations by proposal ID\n",
    "\n",
    "# Convert date string to MJD\n",
    "def mjd_to_str(t):\n",
    "    \"\"\"Converts Modified Julien Date (MJD) input (t) \n",
    "    to a readable date string (t_str).\"\"\"\n",
    "    t_mjd = Time(t, format='mjd')\n",
    "    t_str = t_mjd.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return t_str\n",
    "\n",
    "# Select all science observations for the proposal ID \n",
    "sciobs = Observations.query_criteria(intentType='science', proposal_id='{}'.format(PID))\n",
    "# See available columns in the result\n",
    "print(sciobs.columns)\n",
    "\n",
    "# Convert astropy table to a dataframe for manipulation\n",
    "so_df = pd.DataFrame(np.array(sciobs))\n",
    "\n",
    "# Get the easily readable date string from the MJD dates\n",
    "so_df['t_min_str'] = so_df['t_min'].apply(mjd_to_str)\n",
    "so_df['t_max_str'] = so_df['t_max'].apply(mjd_to_str)\n",
    "\n",
    "# Print just the ASN numbers (obs_id), start and end times (t_min, t_max) in MJD and string form in descending date order\n",
    "so_df[['obs_id', 'target_name', 'instrument_name', 't_min','t_min_str','t_max','t_max_str']].sort_values('t_min', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-simple",
   "metadata": {},
   "source": [
    "**c) Download the data and organize**\n",
    "\n",
    "If there are duplicate FLCs showing in the `mastflcs` list, `astroquery` download has caching so it won't actually download more than one of the FLCs with the same name. FLCs can be listed under different observation IDs (`parent_obsid`) so may listed more than once in `astroquery` but not on MAST that does not filter by that information.\n",
    "\n",
    "Check print command to see if the WFC3/UVIS data is processed with the new calibration codes (`calwf3 v3.6.0(Dec-31-2020)` and above) prior to downloading (set `download=False` above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_dload_data(DLD_DIR, ALL_DIR, ext='_flc.fits'):\n",
    "    \"\"\"Copies the downloaded files out of the nested astroquery directories \n",
    "    and into one combined directory (raw downloaded data, RWD_DIR). Here, \n",
    "    the CTE correction code copies over only the darks with the right exposure \n",
    "    times to be CTE corrected. Or calwf3 is run on all the raw science data files.\n",
    "    \"\"\"\n",
    "    # Move to the download directory\n",
    "    os.chdir(DLD_DIR)\n",
    "\n",
    "    # Copies data out of the astroquery sub-directories and into the RWD_DIR directory if empty\n",
    "    print('Copying downloaded raws from astroquery subdirectories {}'.format(DLD_DIR))\n",
    "    n=0\n",
    "    \n",
    "    for i, idob in enumerate(glob.glob('*')):\n",
    "        # Check if the file is already in the directory, if not it is copied\n",
    "        src = os.path.join(DLD_DIR, idob, idob +ext)\n",
    "        dst = os.path.join(ALL_DIR, idob +ext)\n",
    "        if not os.path.exists(dst):\n",
    "            print('Copying {} to {}'.format(src, dst))\n",
    "            shutil.copy(src, dst)\n",
    "            n+=1\n",
    "        else: \n",
    "            print('File exists {}, not copying file from {}'.format(dst, DLD_DIR))\n",
    "\n",
    "    print('Copied {} files to combined raw data directory {}'.format(n, ALL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-characteristic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search for files with astroquery by instrument and PID (specfied in section 1a) \n",
    "if inst=='WFC3': sciobs = Observations.query_criteria(intentType='science', instrument_name=\"WFC3/UVIS\", proposal_id='{}'.format(PID)) \n",
    "elif inst=='ACS': sciobs = Observations.query_criteria(intentType='science', instrument_name=\"ACS/WFC\", proposal_id='{}'.format(PID))\n",
    "\n",
    "# Get the FLCs for each instrument\n",
    "sciprod = Observations.get_product_list(sciobs)\n",
    "mastflcs = Observations.filter_products(sciprod, productSubGroupDescription=\"FLC\", type='S')\n",
    "print('{} FLCs found:'.format(len(mastflcs)))\n",
    "mastflcs['productFilename', 'project', 'prvversion'].pprint(max_lines=100)\n",
    "\n",
    "# If download option is set, check if the download directory exists, if not then download\n",
    "MDLD_DIR = os.path.join(FLC_DIR, 'mastDownload', 'HST')\n",
    "if download==True:\n",
    "    if os.path.exists(MDLD_DIR):\n",
    "        print('Download directory exists! Not downloading files')\n",
    "    else: \n",
    "        # Move to FLC directory and download FLCs with astroquery\n",
    "        os.chdir(FLC_DIR)\n",
    "        Observations.download_products(mastflcs, mrp_only=False) \n",
    "        \n",
    "        # Rename the download directory to add the PID and date\n",
    "        now = Time.now()\n",
    "        pnow = now.strftime('%Y%b%d')\n",
    "        DLD_DIR = MDLD_DIR.replace('mastDownload', 'mastDownload_PID{}_{}_{}'.format(PID, inst, pnow))\n",
    "        os.rename(os.path.dirname(MDLD_DIR), os.path.dirname(DLD_DIR))\n",
    "        print('Download to {} complete'.format(DLD_DIR))\n",
    "        \n",
    "# Copy files out of the sub-directories into a combined directory\n",
    "if copy==True:\n",
    "    if \"DLD_DIR\" in locals(): copy_dload_data(DLD_DIR, ALL_DIR, ext='_flc.fits')\n",
    "    else: print('WARNING: Download directory (DLD_DIR) not set, set download=True or manually set in Step 1a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-cheese",
   "metadata": {},
   "source": [
    "**d) Optional data check** \n",
    "\n",
    "Check the headers of the downloaded data for more information on the observations and processing. Check if the downloaded WFC3/UVIS data is processed with the new calibration codes (`calwf3 v3.6.0(Dec-31-2020)` and above) and have time-dependent photometry info in their headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into the update WCS directory\n",
    "os.chdir(ALL_DIR)\n",
    "\n",
    "# List flcs\n",
    "files = sorted(glob.glob('*_flc.fits'))\n",
    "\n",
    "# Get FLC and processing information\n",
    "for f in files:\n",
    "    print('--------------------------------------------------------------')\n",
    "    # Open file header\n",
    "    hdr = fits.getheader(f, 0)\n",
    "    \n",
    "    # Get FLC info\n",
    "    print('FILE: {}, INST: {}, DATE OBS:{}'.format(f, hdr['INSTRUME'], hdr['DATE-OBS']))\n",
    "\n",
    "    # Get processing info\n",
    "    if 'WFC3' in hdr['INSTRUME']: caltype='CALWF3'\n",
    "    elif 'ACS' in hdr['INSTRUME']: caltype='CALACS'\n",
    "    print('DATE PROCESSED: {}, {} VERSION: {}'.format(hdr['DATE'], caltype, hdr['CAL_VER']))\n",
    "    \n",
    "    # Check for time-dependent phototmetry header updates in WFC3/UVIS files only\n",
    "    if '2020 Time-dependent Inverse Sensitivity' in str(hdr): timedep='True'\n",
    "    else: timedep='False'\n",
    "    if 'WFC3' in hdr['INSTRUME']: print('Includes new WFC3/UVIS time-dependent photometry: {}'.format(timedep))\n",
    "    print('CTE CORRECTED DARK (DKC): {}'.format(hdr['DRKCFILE']))\n",
    "    print('--------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-cement",
   "metadata": {},
   "source": [
    "# 2) Apply WFC3/UVIS FLC Corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-providence",
   "metadata": {},
   "source": [
    "**a) Flag hot pixels with a variable threshold**\n",
    "\n",
    "i) Set the hot pixel correction directory and copy FLCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set correction subdirectory \n",
    "HP_DIR = os.path.join(COR_DIR, 'hpix_flcs')\n",
    "\n",
    "# Directory is made and files copied over to be corrected\n",
    "cf.copy_files_check(ALL_DIR, HP_DIR, files='*_flc.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-highlight",
   "metadata": {},
   "source": [
    "ii) For each FLC, get their superdarks from the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each FLC, get their filename and corresponding superdark from header\n",
    "flcs = glob.glob(os.path.join(HP_DIR, '*flc.fits'))\n",
    "filenames = []\n",
    "superdarks = []\n",
    "for f in flcs:\n",
    "    filenames.append(os.path.basename(f))\n",
    "    \n",
    "    # Open file header\n",
    "    hdr = fits.getheader(f, 0)\n",
    "    \n",
    "    # Get CTE-corrected dark from header\n",
    "    sdark = hdr['DRKCFILE'].split('iref$')[-1]\n",
    "    superdarks.append(sdark)\n",
    "    \n",
    "# Save files and superdarks to data frame\n",
    "df = pd.DataFrame({})\n",
    "df['file'] = filenames\n",
    "df['superdark'] = superdarks\n",
    "\n",
    "# Get a list of the unique superdarks needed for the FLCs\n",
    "print('Unique superdarks: {}'.format(df['superdark'].unique()))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-surprise",
   "metadata": {},
   "source": [
    "iii) Download the superdarks to the correction directory\n",
    "\n",
    "The files are retrieved from the [HST Calibration Reference Data System (CRDS)](https://hst-crds.stsci.edu/) using the [`crds.sync` function](https://hst-crds.stsci.edu/static/users_guide/command_line_tools.html#crds-sync). If this doesn't work for any reason, download the files directly from CRDS and place them in the `HP_DIR` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into hot pix directory for downloading superdarks\n",
    "os.chdir(HP_DIR)\n",
    "\n",
    "# Set CRDS server   \n",
    "if \"CRDS_SERVER_URL\" not in os.environ:\n",
    "    os.environ['CRDS_SERVER_URL'] = 'https://hst-crds.stsci.edu'\n",
    "# Set the CRDS file cache directory, change if desired, not used if output directory is set\n",
    "if \"CRDS_PATH\" not in os.environ:\n",
    "    os.environ['CRDS_PATH'] = os.path.join(FLC_DIR, 'crds_cache')\n",
    "    \n",
    "# Run script to download superdarks from CRDS\n",
    "for sd in df['superdark'].unique():\n",
    "    print('Downloading superdark {} to {}'.format(sd, HP_DIR))\n",
    "    syncargs = \"crds.sync --hst --files {} --output-dir .\".format(sd)\n",
    "    script = SyncScript(argv=syncargs)\n",
    "    script.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-danish",
   "metadata": {},
   "source": [
    "iv) Flag hot pixels in the superdarks and apply these flags to the FLCs\n",
    "\n",
    "Hot pixels are measured directly from the superdarks. This used to be done with a constant threshold (0.015 e-/s/pixel) which missed ~30% of hot pixels due to the imperfect CTE correction. Instead, a variable threshold is derived from each superdark so that the number of hot pixels found for each chunk of 50 rows roughly matches the value closest to the read out (i.e. the most accurate value). This maximum threshold of 0.015 e-/s/pixel has been well tested by STScI so we keep this as our starting hot pixel threshold by the read out,m but can be changed if you wish.\n",
    "\n",
    "These hot pixel codes were developed by L. Prichard and were used in the previous custom WFC3/UVIS reduction pipeline in this script: [`build_superdarks.py`](https://github.com/lprichard/hst_wfc3_uvis_reduction/tree/master/darks_codes/lp_darks/cal_uvis_make_darks/build_superdarks.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hot_thresh(indata, step_size=50, inc=0.0001, lim='avg', ro_tot=7000, ro_avg=140, max_hot_thresh=0.015, quiet=True):\n",
    "    '''LP added function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    indata : arr\n",
    "        Data array to find hot pixels in and calculate thresholds to find those pixels.\n",
    "    step_size : int\n",
    "        Number of rows to find hot pixels in at a time.\n",
    "    inc : float\n",
    "        Increment to decrease the hot pixel threshold by in order to increase the number of hot pixels.\n",
    "    lim : str\n",
    "        The limit to use to find hot pixels, eithere the total number withing the 50 row steps ``tot`` \n",
    "        or average number per row ``avg``. The average is more stable to outliers\n",
    "    ro_tot : int\n",
    "        The total value of hot pixels close to the read out per step_size.\n",
    "    ro_avg : int\n",
    "        The average number of hot pixels per row close the the read out. \n",
    "    max_hot_thresh : float\n",
    "        The input starting hot pixel threshold (e-/s/pixel) to work downwards from to increase the number of \n",
    "        hot pixels found.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas data frame\n",
    "        A table of starting row number (``row``), hot pixel threshold for the set of \n",
    "        row numbers starting at row (``hot_thresh``), the total number of hotpixels \n",
    "        in that group of rows with values above the hot pixel threshold (``tot_hotpix``),\n",
    "        the average number of hotpixels per row in that group of rows with values above \n",
    "        the hot pixel threshold (``avg_hotpix``).\n",
    "\n",
    "    '''\n",
    "    # There are 2051 rows, the code runs from row 0 to 2050\n",
    "    steps = np.arange(0, indata.shape[0], step_size)\n",
    "\n",
    "    # Storing values\n",
    "    hot_threshs = []\n",
    "    rows=[]\n",
    "    totals=[]\n",
    "    averages=[]\n",
    "\n",
    "    # Loop over each 50 rows to determine number of hot pixels\n",
    "    # Lowers the threshold to match the edge hot pixel values then \n",
    "    # returns the new threshold to meet this count\n",
    "    for step in steps[:-1]:\n",
    "        # Getting chunk of 50 rows, or whatever the step_size values is\n",
    "        dat = indata[step:step+step_size]\n",
    "\n",
    "        # Setting starting hot pixel threshold\n",
    "        ht = max_hot_thresh\n",
    "\n",
    "        # Determining number of hotpixels at the starting threshold\n",
    "        shp, ahp = count_hp(dat, htpix_thresh=ht)\n",
    "        if quiet==False:\n",
    "            print('***************************************')\n",
    "            print('For rows {} to {}'.format(step, step+step_size))\n",
    "            print('Total no. of hotpix: {}'.format(shp))\n",
    "            print('Average no. of hotpix: {}'.format(ahp))\n",
    "\n",
    "        # Reducing the threshold to increase the number of hotpixels \n",
    "        # found if less than the amount close to the readout\n",
    "        if lim=='tot':\n",
    "            while shp<ro_tot:\n",
    "                ht -= inc\n",
    "                shp, ahp = count_hp(dat, htpix_thresh=ht)\n",
    "        elif lim=='avg':\n",
    "            while ahp<ro_avg:\n",
    "                ht -= inc\n",
    "                shp, ahp = count_hp(dat, htpix_thresh=ht)\n",
    "\n",
    "        # If the hot threshold has gone down, set it to the one before it \n",
    "        # went above the maximum no, hot pixels at the edge closest to the \n",
    "        # read out, then store the final values\n",
    "        if ht<0.015:\n",
    "            ht += inc\n",
    "            shp, ahp = count_hp(dat, htpix_thresh=ht)\n",
    "\n",
    "        if quiet==False:\n",
    "            print('FINAL hot pixel threshold: {:.4f}'.format(ht)) \n",
    "            print('FINAL Total no. of hotpix: {}'.format(shp))\n",
    "            print('FINAL Average no. of hotpix: {}'.format(ahp))\n",
    "            print('***************************************')\n",
    "\n",
    "        # Store values\n",
    "        rows.append(step)\n",
    "        hot_threshs.append(ht)\n",
    "        totals.append(shp)\n",
    "        averages.append(ahp)\n",
    "\n",
    "    # Place in a data frame depending on whether it is a total measure of avegrae measure\n",
    "    df = pd.DataFrame({})\n",
    "    df['row']=rows\n",
    "    df['hot_thresh']=hot_threshs\n",
    "    df['tot_hotpix']=totals\n",
    "    df['avg_hotpix']=averages\n",
    "    if quiet==False: print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def count_hp(dat, htpix_thresh=0.015):\n",
    "    '''LP added function:\n",
    "    Counts the number of hotpixels in a given data set and a provided threshold.\n",
    "    Then calculates and returns the total number and average number per row number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : arr\n",
    "        Data array to find hot pixels in\n",
    "    htpix_thresh : float\n",
    "        Pixel vaue to be considered hot, default is 0.015 e-/s/pixel which is the ST standard.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    shp : int\n",
    "        The sum of all the hot pixels within the given data set.\n",
    "    ahp : int\n",
    "        The median per row number of hot pixels within the given data set.\n",
    "    '''\n",
    "    \n",
    "    # Determine which pixels fall below hot pixel threshold\n",
    "    hotpix = dat > htpix_thresh\n",
    "\n",
    "    nhp=[]\n",
    "    # Count the number of hotpix as a function of row number\n",
    "    for i in range(dat.shape[0]):\n",
    "        nhp.append(len(np.where(hotpix[i]==True)[0]))\n",
    "    \n",
    "    # Sum the no. hotpix \n",
    "    shp = np.nansum(nhp)\n",
    "    # Average no. hotpix across the rows\n",
    "    ahp = np.nanmedian(nhp)\n",
    "    \n",
    "    return shp, ahp\n",
    "\n",
    "\n",
    "def fitpix(superdark, max_hot_thresh=0.015, quiet=True):\n",
    "    \"\"\"Function takes full path to a CTE-corrected WFC3/UVIS dark and returns just the hot pixel positions.\n",
    "    These can then be added (bitwise with value of 16) to the DQ arrays of the FLCs.\n",
    "    \n",
    "    You can set the maximum starting threshold (``max_hot_thresh``), default 0.015 e-/s/pixel as this was tested by ST and was the previous \n",
    "    constant threshold for flagging. Set ``quiet=True`` for minimal print statements.\"\"\"\n",
    "\n",
    "    # Open the superdark\n",
    "    hdulist = fits.open(superdark)\n",
    "    \n",
    "    # Get the data\n",
    "    ext1 = hdulist[1].data  #SCI\n",
    "    ext3 = hdulist[3].data  #DQ\n",
    "    ext4 = hdulist[4].data  #SCI\n",
    "    ext6 = hdulist[6].data  #DQ\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # New method of finding pixels that aren't hot (index1/4) \n",
    "    if quiet==False:\n",
    "        print(\"\")\n",
    "        print(\"---------------- Finding hot pixels ----------------\")\n",
    "        print(\"\")\n",
    "    # Calculating the max no. hot pixels at the read out (ro), \n",
    "    # The total (ro_tot) and average (ro_avg) hot pixels per row\n",
    "    # For a chunk 50 rows: first rows of ext1, last rows of ext 4 (there are 2051 rows in each), \n",
    "    # using the tested ST threshold, and the average number of hot pix per row\n",
    "    # There are 2051 rows in ext1, ssize sets the number of rows to consider at a time\n",
    "    ssize=50    #This can be changed if required\n",
    "    steps = np.arange(0, ext1.shape[0], ssize)\n",
    "\n",
    "    if quiet==False:\n",
    "        print(\"Finding hot pixel threshold for {} row chunks to match the average no. hot pixels close to read out\".format(ssize))\n",
    "        print(\"Creating hot pixel threshold as a function of row number\")\n",
    "        print(\"Identifying hot pixels according to the threshold function, mapping onto superdark\")\n",
    "        print(\"For \", superdark)\n",
    "\n",
    "    # Extension 1 maximum values, first ~50 rows\n",
    "    dat1 = ext1[steps[0]:steps[1]]\n",
    "    ro_tot1, ro_avg1 = count_hp(dat1, htpix_thresh=max_hot_thresh)\n",
    "    if quiet==False:\n",
    "        print('Total no. of hotpix rows {}-{} in extension 1: {}'.format(steps[0], steps[1], ro_tot1))\n",
    "        print('Average no. of hotpix rows {}-{} in extension 1: {}'.format(steps[0], steps[1], ro_avg1))\n",
    "\n",
    "    # Extension 4 maximum values, last ~50 rows\n",
    "    dat4 = ext4[steps[-2]:steps[-1]]\n",
    "    ro_tot4, ro_avg4 = count_hp(dat4, htpix_thresh=max_hot_thresh)\n",
    "    if quiet==False:\n",
    "        print('Total no. of hotpix rows {}-{} in extension 4: {}'.format(steps[-2], steps[-1], ro_tot4))\n",
    "        print('Average no. of hotpix rows {}-{} in extension 4: {}'.format(steps[-2], steps[-1], ro_avg4))\n",
    "\n",
    "    # Calculating the hot pixel threshold per 50 rows for each extension\n",
    "    # Hot pixel threshold increment\n",
    "    inc = 0.0001\n",
    "    # Determines whether the limit is the average per row or total numver\n",
    "    lim = 'avg'\n",
    "    # Determines number of rows to measure at a time\n",
    "    df1 = calc_hot_thresh(ext1, step_size=ssize, inc=inc, lim=lim, ro_tot=ro_tot1, ro_avg=ro_avg1, max_hot_thresh=max_hot_thresh, quiet=quiet)\n",
    "    df4 = calc_hot_thresh(ext4, step_size=ssize, inc=inc, lim=lim, ro_tot=ro_tot4, ro_avg=ro_avg4, max_hot_thresh=max_hot_thresh, quiet=quiet)\n",
    "    if quiet==False:\n",
    "        print('Median hot pixels per row for extension 1: {:.4f}'.format(np.nanmedian(df1['avg_hotpix'])))\n",
    "        print('Median hot pixels per row for extension 4: {:.4f}'.format(np.nanmedian(df4['avg_hotpix'])))\n",
    "\n",
    "    # Calculating third order polynomial fits to the hot pixel thresholds for every 50 rows\n",
    "    # Extension 1\n",
    "    fit1 = np.polyfit(df1['row']+(0.5*ssize), df1['hot_thresh'], 3)\n",
    "    p1 = np.poly1d(fit1)\n",
    "\n",
    "    # Extension 4\n",
    "    fit4 = np.polyfit(df4['row']+(0.5*ssize), df4['hot_thresh'], 3)\n",
    "    p4 = np.poly1d(fit4)\n",
    "\n",
    "    # Creating boolean index arrays for good pixels\n",
    "    index1 = np.empty_like(ext1, dtype=bool)\n",
    "    index4 = np.empty_like(ext4, dtype=bool)\n",
    "\n",
    "    # Looping over each 50 rows, finding the average threshold from the polynomial function\n",
    "    # Identifying hot pixels at that threshold, storing in the good pixel array (index1/4)\n",
    "    for step in steps[:-1]:\n",
    "        # For the final step which has one more pixel\n",
    "        if step==steps[-2]:\n",
    "            up = ssize + (ext1.shape[0] % ssize)\n",
    "        else:\n",
    "            up = ssize\n",
    "\n",
    "        # Define rows over which the threshold function will be averaged over\n",
    "        rows=np.arange(step, step+up, 1)\n",
    "\n",
    "        # Set the hot pixel threshold as the average over the 50 row chunk\n",
    "        hot_thresh1 = np.nanmedian(p1(rows))\n",
    "        hot_thresh4 = np.nanmedian(p4(rows))\n",
    "        if quiet==False:\n",
    "            print('Rows {} to {}:'.format(rows[0], rows[-1]))\n",
    "            print('Hot pixel threshold extension 1: {:.4f}'.format(hot_thresh1))\n",
    "            print('Hot pixel threshold extension 4: {:.4f}'.format(hot_thresh4))\n",
    "\n",
    "        # Identify the good pixels in the arrray as below the threshold for each 50 rows\n",
    "        index1[step:step+up] = ext1[step:step+up] <= hot_thresh1\n",
    "        index4[step:step+up] = ext4[step:step+up] <= hot_thresh4\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    # Calculate median and standard deviation\n",
    "    med_ext1 = np.nanmedian(ext1[index1])\n",
    "    std_ext1 = np.nanstd(ext1[index1])\n",
    "    med_ext4 = np.nanmedian(ext4[index4])\n",
    "    std_ext4 = np.nanstd(ext4[index4])\n",
    "\n",
    "    # Set \"good\" pixels to median value\n",
    "    ext1[index1] = med_ext1\n",
    "    ext4[index4] = med_ext4\n",
    "\n",
    "    # Set DQ flags of non-\"good\" pixels to 16 (hot pixel)\n",
    "    hotpix_ext1 = np.where(ext1 != med_ext1)\n",
    "    hotpix_ext4 = np.where(ext4 != med_ext4)\n",
    "    ext3[hotpix_ext1] = 16\n",
    "    ext6[hotpix_ext4] = 16\n",
    "\n",
    "    print(\"Completed fitpix for superdark {}\".format(superdark))\n",
    "    # Save the image\n",
    "    # hdulist.flush()\n",
    "    hdulist.close()\n",
    "\n",
    "    # return hotpix_ext1, hotpix_ext4\n",
    "    return ext3, ext6\n",
    "\n",
    "\n",
    "def flag_flc_hotpix(df, max_hot_thresh=0.015, quiet=False):\n",
    "    \"\"\"Function to flag new hot pixels in the FLCs based on the hot pixel flags found in fitpix().\n",
    "    Takes a pandas dataframe (df) of the FLCs (``file``) and corresponding superdarks (``superdark``).\n",
    "    Adds any new hot pixels flags (value 16) to the DQ array of the FLCs.\"\"\"\n",
    "    \n",
    "    for superdark in df['superdark'].unique():\n",
    "        # Get the dark DQ array with new flagged hot pixels\n",
    "        dkc_ext3, dkc_ext6 = fitpix(superdark, quiet=True)\n",
    "        \n",
    "        for flc in df['file'].loc[df['superdark']==superdark]:\n",
    "\n",
    "            #print('Flagging hot pixels in FLC {}...'.format(flc))\n",
    "            # Open the FLC to update\n",
    "            hdulist = fits.open(flc, mode='update')\n",
    "\n",
    "            # Open the DQ arrays of each FLC chip\n",
    "            flc_ext3 = hdulist[3].data  #DQ\n",
    "            flc_ext6 = hdulist[6].data  #DQ\n",
    "\n",
    "            # Identify the positions of the newly flagged hot pixels\n",
    "            new_hpix_ext3 = np.where(((16 & flc_ext3)==16) != ((16 & dkc_ext3)==16))\n",
    "            new_hpix_ext6 = np.where(((16 & flc_ext6)==16) != ((16 & dkc_ext6)==16))\n",
    "\n",
    "            # Count no. hot pix\n",
    "            oldhp = np.count_nonzero((16 & flc_ext3)==16) + np.count_nonzero((16 & flc_ext6)==16)\n",
    "            newhp = len(new_hpix_ext3[0])+len(new_hpix_ext6[0])\n",
    "            tothp = np.count_nonzero((16 & dkc_ext3)==16) + np.count_nonzero((16 & dkc_ext6)==16)\n",
    "            addhp = 100*(newhp/oldhp)\n",
    "            misshp = 100*(newhp/tothp)\n",
    "            \n",
    "            # Get total number of pixels and ratio of hot pixels\n",
    "            totpix = 2*(flc_ext3.shape[0]*flc_ext3.shape[1])\n",
    "            oldfrac = 100*(oldhp/totpix)\n",
    "            newfrac = 100*(tothp/totpix)\n",
    "\n",
    "            # Add 16 to the bitwise DQ arrays of the FLCs if they don't exist \n",
    "            flc_ext3[new_hpix_ext3] += 16\n",
    "            flc_ext6[new_hpix_ext6] += 16\n",
    "\n",
    "            print('{:.1f}% more ({} new) hot pixel flags added to FLC {}'.format(addhp, newhp, flc))\n",
    "            print('~{:.1f}% missed with previous method'.format(misshp))\n",
    "            print('{:.1f}% total pixels previously flagged increased to {:.1f}%\\n'.format(oldfrac, newfrac))\n",
    "            \n",
    "            # Save the new hot pixels to the FLC\n",
    "            hdulist.flush()\n",
    "            hdulist.close()\n",
    "        print('-------------------------------------------------------\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-adaptation",
   "metadata": {},
   "source": [
    "For each FLC, find the new hot pixels using the derived variable threshold function on the corresponding superdarks. Add these to the FLC DQ arrays if they don't exist and save the FLCs. No changes are saved to the superdarks. Typically flags ~50-60% more hot pixels than were previously flagged. Check flagging with plots below in Step 2a v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into the hot pix directory\n",
    "os.chdir(HP_DIR)\n",
    "\n",
    "# Flag the hot pixels in each FLC as found from their respective superdarks\n",
    "flag_flc_hotpix(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-martin",
   "metadata": {},
   "source": [
    "v) Optional: Check the old and new hot pixel flags\n",
    "\n",
    "The new hot pixels (blue), flagged with a variable threshold, should be roughly constant across all rows. The old method, using a constant threshold, missed increasingly more hot pixels further from the read out (left and right of plot), i.e. were lowest at the chip gap (center of plot, dashed line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first FLC and set paths to the downloaded and corrected FLCs\n",
    "# For ONE FLC, comment ``for flc in df['file']:`` and unindent below\n",
    "flc = df['file'][0]\n",
    "\n",
    "# For ALL FLCs\n",
    "for flc in df['file']:\n",
    "    old_flc = os.path.join(ALL_DIR, flc)\n",
    "    new_flc = os.path.join(HP_DIR, flc)\n",
    "\n",
    "    # Load up the DQ arrays\n",
    "    oldhdul = fits.open(old_flc)\n",
    "    newhdul = fits.open(new_flc)\n",
    "\n",
    "    # Open the DQ arrays of the FLCs\n",
    "    oflc_ext3 = oldhdul[3].data  #DQ\n",
    "    oflc_ext6 = oldhdul[6].data  #DQ\n",
    "    nflc_ext3 = newhdul[3].data  #DQ\n",
    "    nflc_ext6 = newhdul[6].data  #DQ\n",
    "\n",
    "    # Count the number of hot pixels per row\n",
    "    ohp3=[]\n",
    "    ohp6=[]\n",
    "    nhp3=[]\n",
    "    nhp6=[]\n",
    "    for i in range(oflc_ext3.shape[0]):\n",
    "        ohp3.append(np.count_nonzero((16 & oflc_ext3[i])==16))\n",
    "        ohp6.append(np.count_nonzero((16 & oflc_ext6[i])==16))\n",
    "        nhp3.append(np.count_nonzero((16 & nflc_ext3[i])==16))\n",
    "        nhp6.append(np.count_nonzero((16 & nflc_ext6[i])==16))\n",
    "\n",
    "    # Combine hotpix for both chips\n",
    "    ohp = ohp3 + ohp6\n",
    "    nhp = nhp3 + nhp6\n",
    "\n",
    "    # Set figure options\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.rcParams.update({'font.family': 'sans-serif'})\n",
    "\n",
    "    # Plotting hot pixels with the old and new method\n",
    "    plt.plot(smooth(ohp,19), 'crimson', lw=1, label=r'Old hot pixels with constant threshold')\n",
    "    plt.plot(smooth(nhp,19), 'b', lw=1, label=r'New hot pixels with variable threshold')\n",
    "\n",
    "    # Plot chip divide line\n",
    "    plt.axvline(len(ohp3), ls=\"--\", color='k', label='Chip divide')\n",
    "\n",
    "    # Plot parameters\n",
    "    plt.ylim(50,235)\n",
    "    plt.xlim(0,len(ohp))\n",
    "    plt.title(flc)\n",
    "    plt.ylabel('No. hot pixels')\n",
    "    plt.xlabel('Row number')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-indian",
   "metadata": {},
   "source": [
    "**b) Equalize amp offsets**\n",
    "\n",
    "This step equalizes the amplifiers (four quadrants) on WFC3/UVIS FLCs. The default behavior of this code measures the median of each amp, multiplies it by the flat, subtracts that from each amp, and equalizes the amps to the average amp value. This removes bias offsets between the quadrants to produce smoother images. The corrected output FLCs are the `*_flc_eq.fits` files.\n",
    "\n",
    "The code [`make_uvis_skydark.py`](https://github.com/bsunnquist/uvis-skydarks/blob/master/make_uvis_skydark.py) used for this step was developed by Ben Sunnquist. A copy of the code, downloaded from GitHub 17 May 2021 (last updated  Apr 19, 2021), is included in this directory but check the link to Ben's GitHub code for the latest version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "if inst=='WFC3':\n",
    "    # Set correction subdirectory \n",
    "    EQ_DIR = os.path.join(COR_DIR, 'eq_flcs')\n",
    "\n",
    "    # Directory is made and files copied over to be corrected\n",
    "    cf.copy_files_check(ALL_DIR, EQ_DIR, files='*_flc.fits')\n",
    "\n",
    "    # Copy over the make_uvis_skydark.py code from the code directory to the FLC correction directory\n",
    "    cf.copy_files_check(CODE_DIR, EQ_DIR, files='make_uvis_skydark.py')\n",
    "\n",
    "    # Move into the correction directory\n",
    "    os.chdir(EQ_DIR)\n",
    "\n",
    "    # Run the amp offset code\n",
    "    %run make_uvis_skydark.py\n",
    "else: print('Set inst=WFC3 and ensure you have the WFC3 FLCs to apply this correction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-bibliography",
   "metadata": {},
   "source": [
    "**c) Correct for read out cosmic rays (ROCRs)**\n",
    "\n",
    "Applying the ROCR correction to FLCs is a multi-stage process that changes more information than necessary on the FLCs. Therefore, multiple copies of the FLCs are made to ensure that the only thing changed on the final clean output FLCs is an updated data quality (DQ) array. \n",
    "\n",
    "The processed FLCs are ran through a WCS update (with `updatewcs`) of the header so that they can be run through `astrodrizzle` grouped by association number. This is done to create cosmic ray maps/flags used in the ROCR correction. The flagging of ROCR pixels is then performed on the DQ arrays of those FLCs processed with `astrodrizzle`. These DQ arrays are then copied back into the untouched clean copy of the FLCs.  \n",
    "<br />\n",
    "\n",
    "i) Copy and rename files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-positive",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set two new directories for updating WCS and for the final clean ROCR corrected FLCs\n",
    "WCS_DIR = os.path.join(COR_DIR, 'wcs_updt')      # For the FLCs to be processed\n",
    "ROCR_DIR = os.path.join(COR_DIR, 'rocr_clean')   # For the final clean FLCs that will have updated DQ arrays\n",
    "\n",
    "# Creates directories, copies, and renames files if they don't exist\n",
    "cf.copy_files_check(EQ_DIR, WCS_DIR, files='*flc_eq.fits', rename=True, src_str='flc_eq', dst_str='flc')\n",
    "cf.copy_files_check(EQ_DIR, ROCR_DIR, files='*flc_eq.fits', rename=True, src_str='flc_eq', dst_str='flc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-substitute",
   "metadata": {},
   "source": [
    "ii) Update WCS (to avoid `astrodrizzle` errors) and move the updated files to a drizzle directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into the update WCS directory\n",
    "os.chdir(WCS_DIR)\n",
    "\n",
    "# List flcs\n",
    "files = sorted(glob.glob('*_flc.fits'))\n",
    "\n",
    "# Update WCS in header to avoid errors with astrodrizzle\n",
    "updatewcs.updatewcs(files, use_db=False)    #use_db=False for use w drizzlepac 3.1.6 and above\n",
    "\n",
    "# Set the drizzle directory\n",
    "DRIZ_DIR = os.path.join(COR_DIR, 'rocr_driz')\n",
    "\n",
    "# Makes destination directory if it doesn't exist, checks if files exist, copies them if not\n",
    "cf.copy_files_check(WCS_DIR, DRIZ_DIR, files='*flc.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-length",
   "metadata": {},
   "source": [
    "iii) Get batches of FLC based on association (ASN i.e. observing group) number and run them through `astrodrizzle`\n",
    "\n",
    "`astrodrizzle` creates cosmic ray maps in the DQ arrays of each FLC. The drizzles themselves are not used but the FLCs that are edited by `astrodrizzle` are. Only the basic parameters with CR flagging are used for the drizzle.\n",
    "\n",
    "NOTE: The `driz_cr_snr` should be set to best suit your data. Tips from Ben Sunnquist: \"[The ROCR correction code (step v)] typically flagged an additional ~ 5000-50,000 pixels in each chip [(this is printed out in step v)]. If you find it's over/under flagging... you could raise/lower the sigma in the ROCR code [(step e)], or set the `driz_cr_snr='5 4'` [(or higher, below in this drizzle step)] rather than `'3.5 3'` when making the cosmic ray maps, which sometimes over flags CRs (and thus can over flag ROCRs as well). To verify, I blink the FLC SCI (ext=1 & 4) and DQ (ext=3 & 6) extensions, and make sure the negative tails attached to some CRs are flagged.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into the drizzle directory\n",
    "os.chdir(DRIZ_DIR)\n",
    "\n",
    "# Get all FLCs\n",
    "files = sorted(glob.glob('*_flc.fits'))\n",
    "\n",
    "# Loop to get batches of all files to run through astrodrizzle based on ASN ID\n",
    "fields = []\n",
    "asns_full = []\n",
    "asns = []\n",
    "for f in files:\n",
    "\n",
    "    #Read in header for each file and get field and ASN ID\n",
    "    h = fits.open(f)\n",
    "    field = h[0].header['TARGNAME']\n",
    "    asn = h[0].header['ASN_ID']\n",
    "\n",
    "    # For each ASN ID, store the full ASN ID, file abreviation, and fields/target names of observations\n",
    "    if asn not in asns_full:\n",
    "        asns_full.append(asn)\n",
    "        asns.append(f[0:6])\n",
    "        fields.append(field)  \n",
    "\n",
    "print('Unique ASNs: {}'.format(asns_full))\n",
    "print('Unique ASN filenames: {}'.format(asns))\n",
    "print('Fields: {}'.format(fields))\n",
    "\n",
    "# Create lists of files associated with each ASN ID:\n",
    "lists = []\n",
    "for asn in asns:\n",
    "    asn_files = [files[i] for i, s in enumerate(files) if asn in s]\n",
    "    lists.append(asn_files)\n",
    "\n",
    "print(' Lists of files for each ASN ID that will be ran by astrodrizzle in batches:')\n",
    "print(lists)\n",
    "\n",
    "# Get versions\n",
    "teal.unlearn('astrodrizzle')\n",
    "print('Python version {}'.format(python_version()))\n",
    "ad.__version__\n",
    "\n",
    "# Timestamp for drizzles\n",
    "now = datetime.datetime.now()\n",
    "print('*****************************************************************************')\n",
    "print(DRIZ_DIR)\n",
    "print('Drizzle started at ', now.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "print('*****************************************************************************\\n\\n')\n",
    "\n",
    "# Run astrodrizzle with lists and ASN IDs defined above\n",
    "for l, asn in zip(lists, asns):\n",
    "    ad.AstroDrizzle(l, \n",
    "        driz_cr_corr=True, \n",
    "        driz_combine=True,\n",
    "        preserve=False,  \n",
    "        clean=True, \n",
    "        build=True, \n",
    "        driz_cr_snr='3.5 3.0',    # Set this option to best suit your data ('5.0 4.0' or higher), see notes above\n",
    "        output='{}'.format(asn))\n",
    "\n",
    "# Timestamp for drizzles\n",
    "now = datetime.datetime.now()\n",
    "print('\\n\\n*****************************************************************************')\n",
    "print(DRIZ_DIR)\n",
    "print('Drizzle complete at ', now.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "print('*****************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-chair",
   "metadata": {},
   "source": [
    "iv) If desired, make an additional copy of the drizzled, but not yet ROCR corrected, FLCs\n",
    "\n",
    "The drizzle can take a while depending on your data set. If you would like to test the ROCR flagging parameters on a clean set of drizzled FLCs each time, then make an extra copy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes the directory if it doesn't exist, checks for files, copies them over if not there\n",
    "cf.copy_files_check(DRIZ_DIR, DRIZ_DIR.replace('rocr_driz', 'PRErocr_driz'), files='*flc.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-acoustic",
   "metadata": {},
   "source": [
    "v) Run the ROCR corrections\n",
    "\n",
    "This code was developed by Ben Sunnquist and the original version is here: [flag_rocrs.ipynb](https://github.com/bsunnquist/uvis-skydarks/blob/master/flag_rocrs.ipynb). Check there for the latest version. \n",
    "\n",
    "I made some small edits (denoted with LP in the comments) so that it would be compatible with `Python v3.7`/`astropy v4.0` (the original works with `Python v3.6` and `astropy v<4.0`).\n",
    "\n",
    "Tips from Ben Sunnquist: \"[The ROCR correction code (this step)] typically flagged an additional ~ 5000-50,000 pixels in each chip [(this is printed out)].\" Adjust the threshold parameter below or adjust `driz_cr_snr` in step iii to get this level of flagging. To check outputs: \"blink the FLC SCI (ext=1 & 4) and DQ (ext=3 & 6) extensions, and make sure the negative tails attached to some CRs are flagged.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag pixels as \"bad detector pixel\" (DQ value 4) that are within 5 pixels of a CR hit (away from the\n",
    "# readout direction) AND X sigma below the image mean (where the sigma and mean here are from a Gaussian \n",
    "# fit to the sigma-clipped image data).\n",
    "#\n",
    "# These pixels are read out cosmic rays (ROCRs), i.e. CRs that fall during readout and therefore \n",
    "# trick the CTE correction into over correcting them since it thinks they fell farther from the readout\n",
    "# than they actually did.\n",
    "\n",
    "\n",
    "###################################### USER INPUTS ######################################\n",
    "# LP added: Move into the drizzle directory\n",
    "os.chdir(DRIZ_DIR)\n",
    "\n",
    "# the files to use to find the ROCRs (i.e. drizzling has been done on these files so they have CR flags)\n",
    "files = sorted(glob.glob('*_flc.fits'))  # the files to flag ROCRs in\n",
    "\n",
    "# the directory containing the files to add the ROCR flags to (no drizzling has been done on these files)\n",
    "# LP edit: set to the pre-made clean FLC directory\n",
    "untouched_files_dir = ROCR_DIR\n",
    "\n",
    "# the sigma to use when determining the threshold for flagging ROCRs\n",
    "sigma = 2.75   # LP note: adjust to flag the appropriate no. of ROCR pixels for your data\n",
    "#########################################################################################\n",
    "\n",
    "for f in files:\n",
    "    basename = os.path.basename(f)\n",
    "    print('Flagging ROCRs in {} ...'.format(basename))\n",
    "    untouched_file = os.path.join(untouched_files_dir, basename)\n",
    "    h = fits.open(f)\n",
    "    h_untouched = fits.open(untouched_file)\n",
    "\n",
    "    for ext in [1,4]:\n",
    "        data = h[ext].data\n",
    "        dq = h[ext+2].data\n",
    "        dq_untouched = h_untouched[ext+2].data\n",
    "\n",
    "        # Find lower limit for flaggincg ROCRs\n",
    "        clipped = sigma_clip(data, sigma=3, maxiters=5)\n",
    "        d = clipped[clipped.mask==False].data\n",
    "        n, bins = np.histogram(d, bins=70)\n",
    "        bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "        #LP added [0:1] indexes\n",
    "        g_init = models.Gaussian1D(amplitude=np.array(n[n==max(n)][0:1]), mean=np.array(bin_centers[n==max(n)][0:1]), stddev=np.std(d))\n",
    "        \n",
    "        fit_g = fitting.LevMarLSQFitter()\n",
    "        g = fit_g(g_init, bin_centers, n)\n",
    "        # LP removed [0] from g.mean.value\n",
    "        thresh = g.mean.value - sigma*g.stddev.value\n",
    "        print('\\t Threshold Ext {} = {:.3f} - {}*{:.3f} = {:.3f}'.format(ext, g.mean.value, sigma, g.stddev.value, thresh)) \n",
    "        \n",
    "        # Make mask of all CR hits\n",
    "        cr_mask = np.zeros(dq.shape, dtype=int)\n",
    "        cr_mask[dq&4096!=0] = 1\n",
    "\n",
    "        # Flag pixels within 5 pixels of a CR hit (away from readout) that are below the threshold \n",
    "        coords = np.where(cr_mask==1)\n",
    "        cr_mask_new = np.zeros(cr_mask.shape)\n",
    "        for i in np.arange(len(coords[0])):\n",
    "            x,y = coords[1][i], coords[0][i]\n",
    "\n",
    "            # Get the first y-coordinate to check\n",
    "            if ext==1:\n",
    "                running_y = y + 1\n",
    "            elif ext==4:\n",
    "                running_y = y - 1\n",
    "            else:\n",
    "                print('extension {} not expected'.format(ext))\n",
    "\n",
    "            # See if this coordinate has a value below the threshold\n",
    "            count = 0\n",
    "            while count < 5:  # stay within 5 pixels of cr hit\n",
    "                if (running_y <= 2050) & (running_y >= 0):  # avoid going off the image y-dimension          \n",
    "                    val = data[running_y, x]\n",
    "                    if val < thresh:\n",
    "                        cr_mask_new[running_y, x] = 1\n",
    "                    if ext==1:\n",
    "                        running_y += 1\n",
    "                    elif ext==4:\n",
    "                        running_y -= 1\n",
    "                    else:\n",
    "                        print('extension {} not expected'.format(ext))\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                count += 1\n",
    "\n",
    "        # Add in new ROCR flags as 4 (bad detector pixel)\n",
    "        dq_untouched[(dq_untouched&4==0) & (cr_mask_new==1)] += 4\n",
    "        h_untouched[ext+2].data = dq_untouched\n",
    "\n",
    "        # Write out ROCR flag map\n",
    "        fits.writeto(f.replace('_flc.fits','_rocr_map_ext_{}.fits'.format(ext)), cr_mask_new, overwrite=True)\n",
    "        print('\\t # of ROCR flags in Ext {}: {}'.format(ext, len(cr_mask_new[cr_mask_new==1])))\n",
    "\n",
    "    # Write out the ROCR-flagged flc file\n",
    "    outname = untouched_file.replace('_flc.fits','_rocr_flagged_flc.fits')\n",
    "    h_untouched.writeto(outname, overwrite=False)\n",
    "    h_untouched.close()\n",
    "    print('\\t ROCR-flagged image saved to {}'.format(os.path.basename(outname)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-floor",
   "metadata": {},
   "source": [
    "vi) Set a final directory, copy and rename FLCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set final direcotry\n",
    "FIN_DIR = os.path.join(FLC_DIR, 'final_flcs')\n",
    "\n",
    "# Makes destination directory, checks if files exist, copies them if they don't, and renames files as specified\n",
    "cf.copy_files_check(ROCR_DIR, FIN_DIR, files='*_rocr_flagged_flc.fits', rename=True, src_str='rocr_flagged_flc', dst_str='flc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-armor",
   "metadata": {},
   "source": [
    "vii) Update headers of final corrected FLCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into directory\n",
    "os.chdir(FIN_DIR)\n",
    "\n",
    "# Set time now\n",
    "now = Time(Time.now(), format='iso')\n",
    "\n",
    "# List flcs\n",
    "flcs = glob.glob('*flc.fits')\n",
    "\n",
    "# update filenames, history, date\n",
    "n=0\n",
    "for f in flcs:\n",
    "    print('Opening and updating {}...'.format(f))\n",
    "\n",
    "    # Open file for editing\n",
    "    h = fits.open(f)\n",
    "    \n",
    "    # Update header with corrections performed\n",
    "    h[0].header['HISTORY'] = 'FLC corrections (from L Prichard) performed {}'.format(now)\n",
    "    h[0].header['HISTORY'] = 'Code: https://github.com/lprichard/HST_FLC_corrections.ipynb'\n",
    "    h[0].header['HISTORY'] = 'Includes new hot pixel flags (by L Prichard)'\n",
    "    h[0].header['HISTORY'] = 'Includes FLC amp equalization (by B Sunnquist)'\n",
    "    h[0].header['HISTORY'] = 'Code: https://github.com/bsunnquist/uvis-skydarks/blob/master/make_uvis_skydark.py'\n",
    "    h[0].header['HISTORY'] = 'Includes ROCR corrections (by B Sunnquist)'\n",
    "    h[0].header['HISTORY'] = 'Code: https://github.com/bsunnquist/uvis-skydarks/blob/master/flag_rocrs.ipynb'\n",
    "\n",
    "    h.writeto(f, overwrite=True)\n",
    "    h.close()\n",
    "    n+=1\n",
    "    print('Updated header for {}/{}: {}'.format(n, len(flcs), f))\n",
    "\n",
    "print('Updated headers for {} final flcs in {}'.format(n, FIN_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-angle",
   "metadata": {},
   "source": [
    "# 3) Apply ACS FLC Corrections\n",
    "\n",
    "**a) Gradient removal and chip equalization for ACS FLCs**\n",
    "\n",
    "Gradients may only exist in a handful of FLCs and may be more likely to have strong gradients if observed in [Continuous Viewing Zones (CZVs)](https://www.stsci.edu/itt/review/cp_primer_cy17/CP_PRIMER/4_Observation_Types2.html). Correction code [`remove_gradients.ipynb`](https://github.com/bsunnquist/uvis-skydarks/blob/master/remove_gradients.ipynb) by Ben Sunnquist, check for latest version. By default, the code only applies corrections to FLCs with gradients larger than 5 electrons (`gradient_threshold=5`). To apply the corrections (gradient removal and chip equalization) to all FLCs (which results in basically the same output for those FLCs least affected), set `gradient_threshold=None`.\n",
    "\n",
    "Examples of the outputs of the code are below:\n",
    "<img src=\"./images/remove_grad_outputs.png\" alt=\"grad\" width=\"400\"/>\n",
    "\n",
    "Notes from Ben Sunnquist on the code functionality and tips for its application:\n",
    "\n",
    "\"The following is the full process used to remove the gradients from the input FLCs:\n",
    "1. Subtract the clipped median to remove the overall background level\n",
    "2. Find the 2D background gradient of #1 using `photutils` 2D median background estimator\n",
    "3. Subtract the gradient found in #2 from the original FLC data to remove the gradient\n",
    "4. Create a source segmap using the image from #3\n",
    "5. Repeat steps 1-3 using the original, untouched FLCs, but this time masking the sources found in #4 when finding the background gradient\n",
    "6. [Equalizing the gradient subtracted chips to the average chip level as for the WFC3/UVIS FLCs]\n",
    "\n",
    "Documentation on the `photutils` background estimator [here](https://photutils.readthedocs.io/en/stable/background.html#d-background-and-noise-estimation) and on the `photutils` image segmentation for source finding [here](https://photutils.readthedocs.io/en/stable/segmentation.html#source-extraction-using-image-segmentation).\n",
    "\n",
    "For each FLC, the code will output a corresponding `*bkg.fits` and `*segmap.fits` image to inspect the quality of the background gradient subtracted and the source finding. If you find remnants of e.g. diffuse, patchy sources showing in the `*bkg.fits` image, I would recommend either increasing the box size in the background estimate, or decreasing the `nsigma` threshold used in the source detection.\"  \n",
    "<br />\n",
    "\n",
    "i) Copy the FLCs to be corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-angola",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if inst=='ACS':\n",
    "    # Set correction subdirectory \n",
    "    EQ_DIR = os.path.join(COR_DIR, 'eq_flcs')\n",
    "\n",
    "    # Directory is made and files copied over (if they don't exist) to be corrected\n",
    "    cf.copy_files_check(ALL_DIR, EQ_DIR, files='*_flc.fits')\n",
    "else: print('Set inst=ACS and ensure you have the ACS FLCs to apply this correction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-librarian",
   "metadata": {},
   "source": [
    "ii) Remove gradients from FLCs and equalize chip levels\n",
    "\n",
    "(Only some minor edits have been made to the original code, denoted by LP in the comments.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes large-scale background gradients from the input flc files, and equalizes the overall\n",
    "# background levels between chips\n",
    "\n",
    "\n",
    "################################# USER INPUTS #################################\n",
    "# LP added: Move into the correction sub-dir\n",
    "os.chdir(EQ_DIR)\n",
    "\n",
    "# The files to correct\n",
    "files = glob.glob('./*flc.fits')\n",
    "\n",
    "# Only those files whose gradients larger than this threshold will be corrected.\n",
    "# Set to None to correct all files regardless of the measured gradient.\n",
    "gradient_threshold = 5.0\n",
    "\n",
    "# The box size to use when creating the 2D background image\n",
    "box_size = (128, 128)\n",
    "\n",
    "# Option to mask sources when finding the background gradient/pedestal level\n",
    "mask_sources = True\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# Remove those files with no gradient from the processing list\n",
    "if gradient_threshold:\n",
    "    files_to_process = []\n",
    "    for f in files:\n",
    "        diffs = []\n",
    "        for ext in [1, 4]:\n",
    "            data = fits.getdata(f, ext)\n",
    "            left, _,  _, right = np.split(data, 4, axis=1)\n",
    "            clipped_left = sigma_clip(left, sigma=3, maxiters=5)\n",
    "            clipped_right = sigma_clip(right, sigma=3, maxiters=5)\n",
    "            med_left = np.nanmedian(clipped_left.data[clipped_left.mask==False])\n",
    "            med_right = np.nanmedian(clipped_right.data[clipped_right.mask==False])\n",
    "            diffs.append(abs(med_left - med_right))\n",
    "        diffs = np.array(diffs)\n",
    "        if len(diffs[diffs > gradient_threshold]) > 0:\n",
    "            files_to_process.append(f)\n",
    "else:\n",
    "    files_to_process = files\n",
    "\n",
    "# LP added files check\n",
    "if len(files_to_process)>0:\n",
    "    \n",
    "    # STEP 1: Remove the large-scale 2D background gradient from each chip\n",
    "    print('STEP 1: Removing 2D gradients from input FLCs...')\n",
    "    for f in files_to_process:\n",
    "        basename = os.path.basename(f)\n",
    "        print('Working on {}:'.format(basename))\n",
    "        h = fits.open(f)\n",
    "        for ext in [1,4]:\n",
    "            print('\\tWorking on extension {}:'.format(ext))\n",
    "            data_orig = np.copy(h[ext].data)\n",
    "            data = h[ext].data\n",
    "\n",
    "            # Subtract off median\n",
    "            clipped = sigma_clip(data, sigma=3, maxiters=5)\n",
    "            data = data - np.nanmedian(clipped.data[clipped.mask==False])\n",
    "\n",
    "            # Find sources in the gradient-removed image\n",
    "            if mask_sources:\n",
    "                print('\\tMaking source segmap...')\n",
    "                s = SigmaClip(sigma=3.)\n",
    "                bkg_estimator = MedianBackground()\n",
    "                bkg = Background2D(data, box_size=box_size, filter_size=(10, 10), \n",
    "                                   sigma_clip=s, bkg_estimator=bkg_estimator, exclude_percentile=15.0)\n",
    "                skydark = bkg.background\n",
    "                data_flat = data_orig - skydark\n",
    "                threshold = detect_threshold(data_flat, nsigma=0.75)\n",
    "                sigma = 3.0 * gaussian_fwhm_to_sigma\n",
    "                kernel = Gaussian2DKernel(sigma, x_size=3, y_size=3)\n",
    "                kernel.normalize()\n",
    "                segm = detect_sources(data_flat, threshold, npixels=5, filter_kernel=kernel)\n",
    "                segmap = segm.data\n",
    "                fits.writeto(f.replace('_flc.fits', '_segmap_ext{}.fits'.format(ext)), \n",
    "                             segmap, overwrite=True)\n",
    "            else:\n",
    "                segmap = np.zeros(data.shape).astype(int)\n",
    "\n",
    "            # Find the background gradient, incorporating the source mask\n",
    "            print('\\tFinding the background gradient...')\n",
    "            s = SigmaClip(sigma=3.)\n",
    "            bkg_estimator = MedianBackground()\n",
    "            mask = (segmap > 0)\n",
    "            bkg = Background2D(data, box_size=box_size, filter_size=(10, 10), \n",
    "                               sigma_clip=s, bkg_estimator=bkg_estimator, mask=mask, exclude_percentile=15.0)\n",
    "            skydark = bkg.background\n",
    "            fits.writeto(f.replace('_flc.fits', '_bkg_ext{}.fits'.format(ext)), \n",
    "                         skydark, overwrite=True)\n",
    "\n",
    "            # Subtract the background gradient from the original image\n",
    "            data_new = data_orig - skydark\n",
    "            h[ext].data = data_new.astype('float32')\n",
    "\n",
    "        h.writeto(f, overwrite=True)\n",
    "        h.close()\n",
    "        print('Finished removing background gradient from {}'.format(basename))\n",
    "       \n",
    "    \n",
    "    # STEP 2: equalize the overall background/pedestal level between chips to the average of the two chips\n",
    "    print('\\nSTEP 2: Equalizing overall background levels in the input FLCs...')\n",
    "    for f in files_to_process:\n",
    "        basename = os.path.basename(f)\n",
    "        print('Working on {}:'.format(basename))\n",
    "        h = fits.open(f)\n",
    "\n",
    "        # Find the overall background levels in each chip\n",
    "        background_levels = []\n",
    "        for ext in [1,4]:\n",
    "            data = np.copy(h[ext].data)\n",
    "\n",
    "            # Mask sources using the previous segmap\n",
    "            if mask_sources:\n",
    "                segmap = fits.getdata(f.replace('_flc.fits', '_segmap_ext{}.fits'.format(ext)))\n",
    "            else:\n",
    "                segmap = np.zeros(data.shape).astype(int)\n",
    "            data[segmap > 0] = np.nan\n",
    "\n",
    "            # Calculate the background level in the chip\n",
    "            clipped = sigma_clip(data, sigma=3, maxiters=5)\n",
    "            background_levels.append(np.nanmedian(clipped.data[clipped.mask==False]))\n",
    "            print('\\tBackground in ext {} = {:0.5f}'.format(ext, np.nanmedian(clipped.data[clipped.mask==False])))\n",
    "\n",
    "        # Equalize the background levels of the two chips to the average of the two\n",
    "        avg_bkg = np.mean(background_levels)\n",
    "        ext1_orig = np.copy(h[1].data)\n",
    "        ext1_diff = background_levels[0] - avg_bkg\n",
    "        ext1_new = ext1_orig - ext1_diff\n",
    "        h[1].data = ext1_new.astype('float32')\n",
    "        ext4_orig = np.copy(h[4].data)\n",
    "        ext4_diff = background_levels[1] - avg_bkg\n",
    "        ext4_new = ext4_orig - ext4_diff\n",
    "        h[4].data = ext4_new.astype('float32')\n",
    "\n",
    "        # Write out the final calibrated file\n",
    "        h.writeto(f, overwrite=True)\n",
    "        h.close()\n",
    "        print('Finished equalizing background levels between chips in {}'.format(basename))\n",
    "        \n",
    "# LP added\n",
    "else: print('{} files with gradient_threshold>{}, no gradients removed'.format(len(files_to_process), gradient_threshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-leonard",
   "metadata": {},
   "source": [
    "iii) Set a final directory and copy FLCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set final direcotry\n",
    "FIN_DIR = os.path.join(FLC_DIR, 'final_flcs')\n",
    "\n",
    "# Makes destination directory, checks if files exist, copies them if they don't, and renames files as specified\n",
    "cf.copy_files_check(ROCR_DIR, FIN_DIR, files='*flc.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-cannon",
   "metadata": {},
   "source": [
    "iv) Update headers of final corrected FLCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into directory\n",
    "os.chdir(FIN_DIR)\n",
    "\n",
    "# Set time now\n",
    "now = Time(Time.now(), format='iso')\n",
    "\n",
    "# List flcs\n",
    "flcs = glob.glob('*flc.fits')\n",
    "\n",
    "# update filenames, history, date\n",
    "n=0\n",
    "for f in flcs:\n",
    "    print('Opening and updating {}...'.format(f))\n",
    "\n",
    "    # Open file for editing\n",
    "    h = fits.open(f)\n",
    "    \n",
    "    # Update header with corrections performed\n",
    "    h[0].header['HISTORY'] = 'FLC corrections (from L Prichard) performed {}'.format(now)\n",
    "    h[0].header['HISTORY'] = 'Code: Code: https://github.com/lprichard/HST_FLC_corrections.ipynb'\n",
    "    h[0].header['HISTORY'] = 'Includes gradient removal & amp equalization (by B Sunnquist)'\n",
    "    h[0].header['HISTORY'] = 'Code: https://github.com/bsunnquist/uvis-skydarks/blob/master/remove_gradients.ipynb'\n",
    "    \n",
    "    h.writeto(f, overwrite=True)\n",
    "    h.close()\n",
    "    n+=1\n",
    "    print('Updated header for {}/{}: {}'.format(n, len(flcs), f))\n",
    "\n",
    "print('Updated headers for {} final flcs in {}'.format(n, FIN_DIR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
